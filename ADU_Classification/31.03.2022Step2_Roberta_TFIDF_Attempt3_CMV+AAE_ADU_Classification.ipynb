{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6234,"status":"ok","timestamp":1649601903653,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"},"user_tz":-120},"id":"HDhcsX3YjBXc","outputId":"e19a8335-c7b5-4bf3-eba3-da81036edae6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}],"source":["import tensorflow as tf\n","# Getting GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19233,"status":"ok","timestamp":1649601922878,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"},"user_tz":-120},"id":"FD_JYSyvjluf","outputId":"e4a3ed62-b2ce-455c-df68-59aca00f0592"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}],"source":["import torch\n","# If a GPU is available\n","if torch.cuda.is_available():    \n","    #set device to GPU   \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If no GPU is available\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":773,"referenced_widgets":["c1ce079b2c144c179386631e3253ef3c","1022484f52c248e09a2944b43f1f9c7d","f8d24ddf39474a18b022cbcbd3234648","70f53776b8b14385a2a293929f2118f1","847a31ae51454fa58bb8dea044ddf0a1","e9298608d59345efa27c48f52522fe4e","99d6706642b242a691162c6809063179","9139b5e5f69847e2a336ebad5ce1f5fc","5016ae1ba9dd4eccb74426e444352e6a","3f432e08e9964e56834ab9e597820eeb","bb876e0dc8744d87bd1eab77fb01ec19","028e97bea1bd41b7801fb06754c4ce2b","d53a5d619c3e4192822a5176a1addc36","717c2879940449eb9544daa07b909a09","698efa6100e0454b87b806dc558637f7","e3e8e9bd90da4e3c9a4f7850c3567dcf","031d45323e61420eaf10ec13747323d6","c2fe4181ff3548a5a8895775329ec48e","0a5799690fb04647a2ed6ab6e35a2bad","6c19b3611f3b478cb52bd48424925adc","174b541734bd448aa83b5059cef6aefc","e814678c4f3a4285b3249a165294ca23","0406697340ad4a4ea15620c6eda51412","47dbc817b6f34efcb1b6d63987bdf7c6","fd1e0a822cf34864b5e592ce7447f80c","f0ef4786f57a4bcaaf170630dd24c5b1","815a8de03dc74718a985f8f1fd1bd858","4e3ff28f3e6544c7a9b9805358ea6118","21dd5e07391a419c8185aa109506ce8b","e44f886a64834c7bbbd2428f042983ed","e07d46edf1604929a977ac1353f330e2","5a9e63c22c414c33a1ad9e5aea1bd6ba","dc8ecbdcbe0349048ae2ee3239a1ef38"]},"executionInfo":{"elapsed":11956,"status":"ok","timestamp":1649601934828,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"},"user_tz":-120},"id":"xciRRSHqjous","outputId":"9ff89392-f08f-4f13-8962-ca8017b43fbe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 8.0 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 7.6 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 61.6 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 50.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 50.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1ce079b2c144c179386631e3253ef3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"028e97bea1bd41b7801fb06754c4ce2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0406697340ad4a4ea15620c6eda51412"}},"metadata":{}}],"source":["#Importing necessary libraries\n","!pip install transformers\n","\n","import re\n","import scipy\n","import pandas         as pd\n","import io\n","import numpy          as np\n","import copy\n","import seaborn        as sns\n","\n","import transformers\n","from transformers                     import  RobertaModel, RobertaTokenizer, AdamW, get_linear_schedule_with_warmup\n","import torch\n","\n","\n","\n","from sklearn.metrics                  import classification_report\n","from sklearn.metrics                  import confusion_matrix\n","from sklearn.feature_extraction.text  import TfidfVectorizer\n","from sklearn.utils                    import class_weight\n","\n","from torch                            import nn, optim\n","from torch.utils                      import data\n","from sklearn.decomposition            import PCA\n","\n","#Seeding for deterministic results\n","RANDOM_SEED = 16\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(RANDOM_SEED)\n","    torch.cuda.manual_seed_all(RANDOM_SEED) \n","    torch.backends.cudnn.deterministic = True  \n","    torch.backends.cudnn.benchmark = False\n","\n","CLASS_NAMES = ['MajorClaim','Claim','Premise']\n","# CLASS_NAMES =['Non-ADU','ADU']\n","\n","MAX_LENGTH = 200                                    \n","BATCH_SIZE = 4\n","EPOCHS = 8\n","HIDDEN_UNITS = 128\n","\n","tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-large')  #Use roberta-large or roberta-base"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"35f6prNBjvw7"},"outputs":[],"source":["#Converting labels to numbers\n","def label_to_int(label):\n","    if label == 1:\n","        return 0\n","    elif label == 2:\n","        return 1\n","    elif label == 3:\n","        return 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5jFbj3Fjzb6"},"outputs":[],"source":["def processStanceData(df):\n","                                                   #Concatenating twitter and reddit data\n","    result1  = df.replace(np.nan, '', regex=True)                               #Getting rid of NaN values\n","\n","    result1['labelValue'] = result1.ADU_Type.apply(label_to_int)     \n","    result1['TextSrcInre']    = result1['Text']\n","    result1['Features']    = result1['Sentence_Label'].str.cat(result1['Paragraph_Label'],sep=\" \") \n","    data = result1[['Text','Topic','TextSrcInre','Para_No','ADU_Type','labelValue','Features']].copy()    \n","\n","\n","#      replyText           - the reply post (whose stance towards the target needs to be learnt)\n","#      replyTextId         - the ID of the reply post\n","#      previousText        - the text to which replyText was replied\n","#      sourceText          - the source post of the conversation thread\n","#      label               - the label value assigned to each post\n","#      previoysPlusSrctext - the concatenation of the previousText and the sourceText\n","#      labelValue          - the numberic value assigned to each label\n","\n","    data.columns = ['Text','Topic','TextSrcInre','Para_No','ADU_Type','labelValue','Features']\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21998,"status":"ok","timestamp":1649601956811,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"},"user_tz":-120},"id":"Vk2N3Axpj2ZE","outputId":"54eb69bb-e049-456f-a581-f708450bdc27"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["# Reading data from AAE premise and claims file as dataFrames\n","\n","# Reading data from AAE premise and claims file as dataFrames\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","path = F\"/content/gdrive/My Drive/Colab Notebooks/1. ADU_Classification/Data/\" \n","# df_temp=pd.read_excel(path,sheet_name='data_AAE_CMV')\n","# # df_temp=pd.read_excel(path,sheet_name='data')\n","# df=df_temp.loc[df_temp['ADU_Type']!='None']\n","# df = df.reset_index(drop=True)\n","# processed_df = processStanceData(df)\n","# from sklearn.model_selection import train_test_split \n","\n","# x, testDf, y, y_test = train_test_split (processed_df,processed_df.labelValue.values, test_size=0.15, train_size=0.85 )\n","# trainDf, devDf, y_train, y_dev = train_test_split(x,y,test_size = 0.17, train_size =0.83)\n","\n","\n","# # plot_df=df['ADU_Type'].value_counts()\n","# # plot_df.plot(kind='bar')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxP62q-fyi2y"},"outputs":[],"source":["# FINAl approach \n","\n","\n","# trainDf=pd.read_csv(path+'model1_train_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","# trainDf=trainDf.loc[trainDf.labelValue!=0]\n","# trainDf=trainDf.drop(columns='Unnamed: 0')\n","# devDf=pd.read_csv(path+'model1_dev_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","# devDf=devDf.drop(columns='Unnamed: 0')\n","# devDf=devDf.loc[devDf.labelValue!=0]\n","# testDf=pd.read_csv(path+'model1_test_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","# testDf=testDf.drop(columns='Unnamed: 0')\n","# testDf=testDf.loc[testDf.labelValue!=0]\n","\n","# trainDf.labelValue=trainDf.labelValue.apply(label_to_int)\n","# devDf.labelValue=devDf.labelValue.apply(label_to_int)\n","# testDf.labelValue=testDf.labelValue.apply(label_to_int)\n","\n","########----------- Model 1 modified  train cmv+aae test on AAE \n","# trainDf=pd.read_csv(path+'model1_train_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","# trainDf=trainDf.loc[trainDf.labelValue!=0]\n","# trainDf=trainDf.drop(columns='Unnamed: 0')\n","# devDf=pd.read_csv(path+'model1_dev_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","# devDf=devDf.drop(columns='Unnamed: 0')\n","# devDf=devDf.loc[devDf.labelValue!=0]\n","# testDf=pd.read_csv(path+'model_2_3_testDf_aae.csv',sep=\"\\t\",index_col=False)\n","# testDf=testDf.drop(columns='Unnamed: 0')\n","# testDf=testDf.loc[testDf.labelValue!=0]\n","\n","# trainDf.labelValue=trainDf.labelValue.apply(label_to_int)\n","# devDf.labelValue=devDf.labelValue.apply(label_to_int)\n","# testDf.labelValue=testDf.labelValue.apply(label_to_int)\n","\n","#----------- Model 2  training data is CMV+AAE , test data is AAE \n","\n","# trainDf=pd.read_csv(path+'model_2_train_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","# trainDf=trainDf.loc[trainDf.labelValue!=0]\n","# trainDf=trainDf.drop(columns='Unnamed: 0')\n","# devDf=pd.read_csv(path+'model_2_dev_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","# devDf=devDf.drop(columns='Unnamed: 0')\n","# devDf=devDf.loc[devDf.labelValue!=0]\n","# testDf=pd.read_csv(path+'model_2_3_testDf_aae.csv',sep=\"\\t\",index_col=False)\n","# testDf=testDf.drop(columns='Unnamed: 0')\n","# testDf=testDf.loc[testDf.labelValue!=0]\n","\n","# trainDf.labelValue=trainDf.labelValue.apply(label_to_int)\n","# devDf.labelValue=devDf.labelValue.apply(label_to_int)\n","# testDf.labelValue=testDf.labelValue.apply(label_to_int)\n","\n","###-------------- MODEL 3 AAE Baseline \n","\n","trainDf=pd.read_csv(path+'model3_trainDf_aae.csv',sep=\"\\t\",index_col=False)\n","trainDf=trainDf.loc[trainDf.labelValue!=0]\n","trainDf=trainDf.drop(columns='Unnamed: 0')\n","devDf=pd.read_csv(path+'model3_devDf_aae.csv',sep=\"\\t\",index_col=False)\n","devDf=devDf.drop(columns='Unnamed: 0')\n","devDf=devDf.loc[devDf.labelValue!=0]\n","testDf=pd.read_csv(path+'model_2_3_testDf_aae.csv',sep=\"\\t\",index_col=False)\n","testDf=testDf.drop(columns='Unnamed: 0')\n","testDf=testDf.loc[testDf.labelValue!=0]\n","\n","trainDf.labelValue=trainDf.labelValue.apply(label_to_int)\n","devDf.labelValue=devDf.labelValue.apply(label_to_int)\n","testDf.labelValue=testDf.labelValue.apply(label_to_int)"]},{"cell_type":"code","source":["trainDf.labelValue.value_counts()"],"metadata":{"id":"DuDn6V1uPNDK","executionInfo":{"status":"ok","timestamp":1649601958843,"user_tz":-120,"elapsed":29,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8a4bbe81-c858-4a3c-f4dd-0a3fd1b65b9c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2    2541\n","1     998\n","0     523\n","Name: labelValue, dtype: int64"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["cmv=trainDf.loc[trainDf.Para_No.isnull()]\n","cmv.labelValue.value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iUmkrz_v-AqC","executionInfo":{"status":"ok","timestamp":1649601958843,"user_tz":-120,"elapsed":21,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"outputId":"28042dbf-03f1-4773-d51c-e4db795ec342"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Series([], Name: labelValue, dtype: int64)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["devDf.labelValue.value_counts()"],"metadata":{"id":"L7gjhnNoPoDJ","executionInfo":{"status":"ok","timestamp":1649601958844,"user_tz":-120,"elapsed":20,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9047024a-8d77-437d-bade-96e3fc714320"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2    434\n","1    175\n","0     97\n","Name: labelValue, dtype: int64"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["cmv_dev=devDf.loc[devDf.Para_No.isnull()]\n","print(cmv_dev.labelValue.value_counts())\n","aae_dev=devDf.loc[~devDf.Para_No.isnull()]\n","print(aae_dev.labelValue.value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F_YL-xt__ulw","executionInfo":{"status":"ok","timestamp":1649601958844,"user_tz":-120,"elapsed":18,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"outputId":"5aad088e-ac44-4620-a623-7e563ce44745"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Series([], Name: labelValue, dtype: int64)\n","2    434\n","1    175\n","0     97\n","Name: labelValue, dtype: int64\n"]}]},{"cell_type":"code","source":["testDf.labelValue.value_counts()"],"metadata":{"id":"C2WbsRIKROHr","executionInfo":{"status":"ok","timestamp":1649601958844,"user_tz":-120,"elapsed":16,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0fd79a50-9d27-4735-ad20-ccf87d9bc63f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2    450\n","1    190\n","0     80\n","Name: labelValue, dtype: int64"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["from numpy.core.arrayprint import printoptions\n","test_cmv=testDf.loc[testDf.Para_No.isnull()]\n","print(test_cmv.labelValue.value_counts())\n","test_aae=testDf.loc[~testDf.Para_No.isnull()]\n","print(test_aae.labelValue.value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hYxb_xGjAVEQ","executionInfo":{"status":"ok","timestamp":1649601958845,"user_tz":-120,"elapsed":14,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"outputId":"82add694-4fee-497d-e806-9f593905d408"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Series([], Name: labelValue, dtype: int64)\n","2    450\n","1    190\n","0     80\n","Name: labelValue, dtype: int64\n"]}]},{"cell_type":"code","source":["print(len(trainDf), len(devDf),len(testDf))\n","print(trainDf.labelValue.value_counts(), devDf.labelValue.value_counts(),testDf.labelValue.value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vg17cWoOlhVt","executionInfo":{"status":"ok","timestamp":1649601958845,"user_tz":-120,"elapsed":12,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"outputId":"fa52a414-05bb-4a5e-ec1e-ccb0b394cc5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4062 706 720\n","2    2541\n","1     998\n","0     523\n","Name: labelValue, dtype: int64 2    434\n","1    175\n","0     97\n","Name: labelValue, dtype: int64 2    450\n","1    190\n","0     80\n","Name: labelValue, dtype: int64\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uv12v5mPkC12"},"outputs":[],"source":["#Creates a dataset which will be used to feed to RoBERTa\n","class StanceDataset(data.Dataset):\n","  def __init__(self,TextSrcInre,Features, labelValue,  tokenizer, max_len):\n","\n","    # def __init__(self,firstSeq,TextSrcInre, labelValue,  tokenizer, max_len):\n","#     def __init__(self, firstSeq, secondSeq, TextSrcInre, labelValue,  tokenizer, max_len):\n","        # self.firstSeq    = firstSeq      #First input sequence that will be supplied to RoBERTa\n","        # self.secondSeq   = secondSeq     #Second input sequence that will be supplied to RoBERTa\n","        self.TextSrcInre = TextSrcInre   #Concatenation of reply+ previous+ src text to get features from 1 training example\n","        self.Features = Features\n","        self.labelValue  = labelValue    #label value for each training example in the dataset\n","        self.tokenizer   = tokenizer     #tokenizer that will be used to tokenize input sequences (Uses BERT-tokenizer here)\n","        self.max_len     = max_len       #Maximum length of the tokens from the input sequence that BERT needs to attend to\n","\n","  def __len__(self):\n","        return len(self.labelValue)\n","\n","  def __getitem__(self, item):\n","        # firstSeq    = str(self.firstSeq[item])\n","        # secondSeq   = str(self.secondSeq[item])\n","        TextSrcInre = str(self.TextSrcInre[item])\n","        Features = str(self.Features[item])\n","\n","    #Encoding the first and the second sequence to a form accepted by RoBERTa\n","    #RoBERTa does not use token_type_ids to distinguish the first sequence from the second sequnece.\n","        encoding = tokenizer.encode_plus(\n","            # firstSeq,\n","            # secondSeq,\n","            TextSrcInre,\n","            Features,\n","            max_length = self.max_len,\n","            add_special_tokens= True,\n","            truncation = True,\n","            pad_to_max_length = True,\n","            # padding=True,\n","            return_attention_mask = True,\n","            return_tensors = 'pt'\n","        )\n","\n","        return {\n","            # 'firstSeq' : firstSeq,\n","            # 'secondSeq' : secondSeq,\n","            'TextSrcInre': TextSrcInre,\n","            'Features' : Features,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labelValue'  : torch.tensor(self.labelValue[item], dtype=torch.long)\n","        }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RlUhuMaekFgW"},"outputs":[],"source":["#Creates a data loader\n","def createDataLoader(dataframe, tokenizer, max_len, batch_size):\n","    ds = StanceDataset(\n","        # firstSeq    = dataframe.Topic.to_numpy(),\n","        # secondSeq   = dataframe.Topic.to_numpy(),\n","        TextSrcInre = dataframe.TextSrcInre.to_numpy(),\n","        Features = dataframe.Features.to_numpy(),\n","        labelValue  = dataframe.labelValue.to_numpy(),\n","        tokenizer   = tokenizer,\n","        max_len     = max_len\n","    )\n","\n","    return data.DataLoader(\n","        ds,\n","        batch_size  = batch_size,\n","        shuffle     = True,\n","        num_workers = 2\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C_1_5LUykFox"},"outputs":[],"source":["#Creating data loader for training data\n","trainDataLoader        = createDataLoader(trainDf, tokenizer, MAX_LENGTH, BATCH_SIZE)\n","\n","#Creating data loader for development data\n","developmentDataLoader  = createDataLoader(devDf, tokenizer, MAX_LENGTH, BATCH_SIZE)\n","\n","#Creating data loader for test data\n","testDataLoader         = createDataLoader(testDf, tokenizer, MAX_LENGTH, BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3583,"status":"ok","timestamp":1649601962421,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"},"user_tz":-120},"id":"_CO-2GC4kFxV","outputId":"894bb0af-6913-4cf2-a5a6-f62a422bab6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["x_train_feats:  TfidfVectorizer(max_df=0.5, min_df=10, ngram_range=(1, 2))\n","length:  1688\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]},{"output_type":"stream","name":"stdout","text":["x_train_transform.shape:  (4062, 1688)\n","PCA(n_components=128)\n","torch.DoubleTensor\n","torch.Size([4062, 128])\n","tensor([[ 0.2431,  0.1241,  0.0140,  ..., -0.0171,  0.0123, -0.0427],\n","        [ 0.0186, -0.0032, -0.0425,  ..., -0.0084,  0.0667, -0.0153],\n","        [ 0.0784, -0.0111,  0.0710,  ...,  0.0408,  0.0496, -0.0245],\n","        ...,\n","        [ 0.0545, -0.0570,  0.0608,  ..., -0.0104,  0.0007,  0.0273],\n","        [-0.0748,  0.0248, -0.0360,  ..., -0.0606, -0.0292, -0.0116],\n","        [ 0.0711,  0.0804, -0.0443,  ...,  0.0020, -0.0127,  0.0397]],\n","       dtype=torch.float64)\n"]}],"source":["#Instantiating the tf-idf vectorizer object\n","tfidf = TfidfVectorizer(min_df = 10, max_df = 0.5, ngram_range=(1,2))\n","\n","x_train = trainDf['TextSrcInre'].tolist()\n","x_train_feats = tfidf.fit(x_train)\n","print('x_train_feats: ',x_train_feats)\n","print('length: ',len(x_train_feats.get_feature_names()))\n","\n","\n","x_train_transform = x_train_feats.transform(x_train)\n","tfidf_transform_tensor = torch.tensor(scipy.sparse.csr_matrix.todense(x_train_transform)).float()\n","print('x_train_transform.shape: ',x_train_transform.shape)\n","\n","\n","pca = PCA(n_components=128)\n","p = pca.fit(tfidf_transform_tensor)\n","# print(p.shape)\n","print(p)\n","X = p.transform(tfidf_transform_tensor)\n","# torch.from_numpy(X.values)\n","X = torch.from_numpy(X)\n","# tfidf_transform_tensor_pca = torch.tensor(scipy.sparse.csr_matrix.todense(X)).float()\n","print(X.type())\n","print(X.shape)\n","print(X)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GWFoeVtrkF4u"},"outputs":[],"source":["#This class defines the model that was used to pre-train a SNN on TF-IDF features\n","class Tfidf_Nn(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        # Inputs to hidden layer linear transformation\n","        self.hidden  = nn.Linear(len(tfidf.get_feature_names()), HIDDEN_UNITS)\n","        # Output layer\n","        self.output  =  nn.Linear(HIDDEN_UNITS, 3)\n","        self.dropout = nn.Dropout(0.1)\n","        \n","        # Defining tanh activation and softmax output \n","        self.tanh = nn.Tanh()\n","        self.softmax = nn.Softmax(dim=1)\n","        \n","    def forward(self, x):\n","        # Pass the input tensor through each of our operations\n","        x = self.hidden(x)\n","        #print(x.shape)\n","        y = self.tanh(x)\n","        #print(y.shape)\n","        z = self.dropout(y)\n","        #print(z.shape)\n","        z = self.output(z)\n","        #print(z.shape)\n","        z = self.softmax(z)\n","        \n","        #Returning the ouputs from the hidden layer and the final output layer\n","        return  y, z\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":547,"status":"ok","timestamp":1649601962960,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"},"user_tz":-120},"id":"oCZQ0OE2k13Q","outputId":"eab1a807-47b0-4a56-f73f-a6643f730983"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]},{"output_type":"execute_result","data":{"text/plain":["Tfidf_Nn(\n","  (hidden): Linear(in_features=1688, out_features=128, bias=True)\n","  (output): Linear(in_features=128, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (tanh): Tanh()\n","  (softmax): Softmax(dim=1)\n",")"]},"metadata":{},"execution_count":20}],"source":["snnmodel = Tfidf_Nn()\n","\n","# model_save_name = 'Mlp_Adu_AAE.pt'\n","model_save_name = 'Mlp_Step2_model_3.pt'\n","path = F\"/content/gdrive/My Drive/Colab Notebooks/1. ADU_Classification/{model_save_name}\"\n","\n","snnmodel.load_state_dict(torch.load(path))\n","snnmodel.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKNTUy8Hk16J"},"outputs":[],"source":["'''This class defines the model that will be used for \n","training and testing on the dataset.\n","\n","Adapted from huggingFace\n","This RoBERTa model from huggingface outputs the last hidden states\n","and the pooled output by default. Pooled output is the classification \n","token (1st token of the last hidden state) further processed by a Linear\n","layer and a Tanh activation function.\n","\n","The pre-trained RoBERTa model is used as the primary model.\n","This class experiments with RoBERTa and its ensemble with TF-IDF features. \n","roberta-only :            No ensembling. This just fine-tunes the RoBERTa model. \n","                          The pooled output is passed through a linear layer and \n","                          softmax function is finally used for preictions. \n","\n","roberta-tfIdf :           This model conatenates the 1st token of last-hidden layer\n","                          from RoBERTa with TF-IDF features. Various ways of this \n","                          concatenation was experimented (using pooled output instead\n","                          of 1st token of last hidden layer etc)\n","\n","roberta-pcaTfidf :        This model concatenates the pooled output from\n","                          RoBERTa with the PCA transformed vector.\n","\n","roberta-preTrainedTfIdf : This model concatenates the pooled output from\n","                          RoBERTa with the hidden layer output from a pre-trained\n","                          SNN that was trained on TF-IDF features.\n","\n","Used dropout to prevent over-fitting.'''\n","\n","class StanceClassifier(nn.Module):\n","\n","    def __init__(self,  n_classes):\n","        super(StanceClassifier, self).__init__()\n","        self.robertaModel              = RobertaModel.from_pretrained('roberta-large')    #use roberta-large or roberta-base\n","        self.model_TFIDF               = snnmodel                                        #Pre-trained SNN trained with TF-IDF features\n","\n","        self.drop                      = nn.Dropout(p = 0.3)\n","\n","        self.output                    = nn.Linear(self.robertaModel.config.hidden_size, n_classes)\n","\n","        self.input_size_tfidf_only     = self.robertaModel.config.hidden_size + len(tfidf.get_feature_names())\n","        self.input_size_tfidf_pca      = self.robertaModel.config.hidden_size + HIDDEN_UNITS\n","\n","        self.dense                     = nn.Linear( self.input_size_tfidf_only,  self.input_size_tfidf_only)\n","        self.out_proj                  = nn.Linear( self.input_size_tfidf_only, n_classes)\n","        self.out_pca                   = nn.Linear( self.input_size_tfidf_pca, n_classes)\n","\n","        self.input_size_preTrain_tfidf = self.robertaModel.config.hidden_size +  HIDDEN_UNITS \n","        self.out                       = nn.Linear(self.input_size_preTrain_tfidf, n_classes)\n","\n","        self.softmax                   = nn.Softmax(dim = 1)\n","\n","    def forward(self, input_ids, attention_mask, inputs_tfidf_feats, pca_transformed_feats, modelType):\n","        roberta_output     = self.robertaModel(\n","            input_ids      = input_ids,               #Input sequence tokens\n","            attention_mask = attention_mask )         #Mask to avoid performing attention on padding tokens\n","    #print(roberta_output[1].shape)\n","        if modelType   == 'roberta-only':\n","            pooled_output = roberta_output[1]           #Using pooled output\n","            output        = self.drop(pooled_output)\n","            output        = self.output(output)\n","\n","        elif modelType == 'roberta-tfIdf':\n","            soutput = roberta_output[1]#---------        experimenting with pooled output \n","            #soutput = roberta_output[0][:, 0, :]        #taking <s> token (equivalent to [CLS] token in BERT)\n","            x       = torch.cat((soutput, inputs_tfidf_feats) , dim=1)\n","            x       = self.drop(x)\n","            output  = self.out_proj(x)\n","\n","        elif modelType == 'roberta-pcaTfidf':\n","            soutput = roberta_output[1]\n","            x       = torch.cat((soutput, pca_transformed_feats) , dim=1)\n","            x       = self.drop(x)\n","            output  = self.out_pca(x)\n","\n","        elif modelType == 'roberta-TrainedTfIdf':\n","            tfidf_hidddenLayer, tfidf_output = self.model_TFIDF(inputs_tfidf_feats)\n","            #print(tfidf_hidddenLayer.shape)\n","            #print(tfidf_output.shape)\n","\n","          #Conactenating pooled output from RoBERTa with the hidden layer from the pre-trained SNN using TF-IDF features. \n","          #pooled_output = torch.cat((roberta_output[1], tfidf_output) , dim=1)-------- Experimenting with Output of pre-trained SNN \n","            pooled_output = torch.cat((roberta_output[1], tfidf_hidddenLayer) , dim=1)\n","            output        = self.drop(pooled_output)\n","            output        = self.out(output)\n","\n","        return self.softmax(output)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["5287facbaed74d6e8e802a2acbded356","a4712244534b4f219b38267ba2960efd","47fe510231284bdf84bdffbcd1545ca5","e73301c7aedb4fc3851041fc1cf874c9","e2383f6ba5494b7b91753b741b36043b","c64363a260824bd9b8c1cef74f698bd6","d49d4477b420422abad96f6cef55e062","0341f6aca88348ff8593ca8fe30226bb","1b456e4165e54f2b81b1b40d235a03a6","c0d5fa872b4f4b9fa0626248ab85431e","90f07865d677456d8205e8ae6755f142"]},"executionInfo":{"elapsed":97539,"status":"ok","timestamp":1649602060495,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"},"user_tz":-120},"id":"uerPBwcIk1_F","outputId":"4637edf7-ea9e-4857-cb86-5ba747fb42f7"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5287facbaed74d6e8e802a2acbded356"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}],"source":["#Instantiating a StanceClassifier object as our model and loading the model onto the GPU.\n","model = StanceClassifier(len(CLASS_NAMES))\n","model = model.to(device)\n","#print(model)"]},{"cell_type":"code","source":["y_train=trainDf.labelValue\n","class_weights = class_weight.compute_class_weight(\n","                                        class_weight = \"balanced\",\n","                                        classes = np.unique(y_train),\n","                                        y = y_train                                                    \n","                                    )\n","# class_weights = dict(zip(np.unique(y_train), class_weights)),\n","class_weights"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"weYxAUmTl34b","executionInfo":{"status":"ok","timestamp":1649602060496,"user_tz":-120,"elapsed":9,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"outputId":"5675efe4-8e70-426b-f122-20ae2b1214b3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2.58891013, 1.35671343, 0.53286108])"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tpzDZjisk2E1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649602060895,"user_tz":-120,"elapsed":407,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"outputId":"f39d9e60-6867-40de-849a-29709689a528"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}],"source":["'''Using the same optimiser as used in BERT paper\n","with a different learning rate'''\n","optimizer = AdamW(model.parameters(), \n","                  lr = 2e-6, \n","                  # lr = 1e-5,\n","                  correct_bias= False)\n","\n","totalSteps = len(trainDataLoader) * EPOCHS\n","\n","scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=0,\n","            num_training_steps = totalSteps\n",")\n","\n","'''Using class-weights to accomodate heavily imbalanced data. \n","These weights were learnt by running several experiments using \n","other weights and the weights that produced the best results have\n","finally been used here'''\n","\n","# weights      = [1.0, 1.0, 1.0, 1.0]\n","weights = class_weights\n","classWeights = torch.FloatTensor(weights)\n","lossFunction = nn.CrossEntropyLoss(weight = classWeights).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xY9qwj8jk2Hg"},"outputs":[],"source":["#This function is used for training the model with 'roberta-TrainedTfIdf'. \n","def train_epoch(\n","  model,\n","  dataLoader,\n","  lossFunction,\n","  optimizer,\n","  device,\n","  scheduler,\n","  n_examples\n","):\n","    model = model.train()\n","    losses = []\n","    correctPredictions = 0\n","\n","    for d in dataLoader:\n","    \n","        input_ids              = d[\"input_ids\"].to(device)                           #Loading input ids to GPU\n","        attention_mask         = d[\"attention_mask\"].to(device)                      #Loading attention mask to GPU\n","        labelValues            = d[\"labelValue\"].to(device)                          #Loading label value to GPU\n","        textSrcInre            = d[\"TextSrcInre\"]\n","        Features               = d[\"Features\"]                                  \n","        tfidf_transform        = x_train_feats.transform(textSrcInre)\n","        tfidf_transform_tensor = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_transform)).float()   \n","        pca_tensor             = p.transform(tfidf_transform_tensor)\n","\n","        pca_tensor = torch.from_numpy(pca_tensor).float()\n","        pca_tensor = pca_tensor.to(device)\n","        tfidf_transform_tensor = tfidf_transform_tensor.to(device)\n","\n","        #Getting the output from our model (Object of StanceClassification class) for train data\n","        outputs = model(\n","          input_ids             = input_ids,\n","          attention_mask        = attention_mask,\n","          inputs_tfidf_feats    = tfidf_transform_tensor,\n","          pca_transformed_feats = pca_tensor,\n","          modelType             = 'roberta-TrainedTfIdf'\n","        )\n","\n","        #Determining the model predictions\n","        _, predictionIndices = torch.max(outputs, dim=1)\n","        loss = lossFunction(outputs, labelValues)\n","\n","        #Calculating the correct predictions for accuracy\n","        correctPredictions += torch.sum(predictionIndices == labelValues)\n","        losses.append(loss.item())\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","    return np.mean(losses), correctPredictions.double() / n_examples\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmKzvuIElHFx"},"outputs":[],"source":["#This function is used for evaluating the model on the development and test set\n","def eval_model(\n","    model, \n","    dataLoader, \n","    lossFunction,\n","    device,\n","    n_examples\n","    ):\n","    model = model.eval()\n","    losses = []\n","    correctPredictions = 0\n","\n","    with torch.no_grad():\n","        for d in dataLoader:\n","            input_ids              = d[\"input_ids\"].to(device)                          #Loading input ids to GPU\n","            attention_mask         = d[\"attention_mask\"].to(device)                     #Loading attention mask to GPU\n","            labelValues            = d[\"labelValue\"].to(device)                         #Loading label values to GPU\n","            textSrcInre            = d[\"TextSrcInre\"]\n","            tfidf_transform        = x_train_feats.transform(textSrcInre)\n","            tfidf_transform_tensor = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_transform)).float()    \n","\n","            pca_tensor             = p.transform(tfidf_transform_tensor)\n","\n","            pca_tensor = torch.from_numpy(pca_tensor).float()\n","            pca_tensor = pca_tensor.to(device)\n","            tfidf_transform_tensor = tfidf_transform_tensor.to(device)\n","\n","            #Getting the softmax output from model for dev data\n","            outputs = model(\n","            input_ids             = input_ids,\n","            attention_mask        = attention_mask,\n","            inputs_tfidf_feats    = tfidf_transform_tensor,\n","            pca_transformed_feats = pca_tensor,\n","            modelType             = 'roberta-TrainedTfIdf'\n","            )\n","\n","            #Determining the model predictions\n","            _, predictionIndices = torch.max(outputs, dim=1)\n","            loss = lossFunction(outputs, labelValues)\n","\n","            #Calculating the correct predictions for accuracy\n","            correctPredictions += torch.sum(predictionIndices == labelValues)\n","            losses.append(loss.item())\n","\n","    return np.mean(losses), correctPredictions.double() / n_examples\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N2tbWMvslJH0","executionInfo":{"status":"ok","timestamp":1649607154451,"user_tz":-120,"elapsed":5093558,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"outputId":"13a07f6a-81cd-4618-a36d-b06da06ef126"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Training loss 0.8964268148649396 Training accuracy 0.7205809945839489\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Development loss 0.8973959051283066 Development accuracy 0.7337110481586403\n","\n","\n","Epoch 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Training loss 0.860638976214439 Training accuracy 0.7395371738060069\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Development loss 0.820307207646343 Development accuracy 0.7407932011331445\n","\n","\n","Epoch 3\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Training loss 0.8144661835211469 Training accuracy 0.7759724273756771\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Development loss 0.8317083312293231 Development accuracy 0.764872521246459\n","\n","\n","Epoch 4\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Training loss 0.7830740063795893 Training accuracy 0.7988675529295914\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Development loss 0.8355828355261161 Development accuracy 0.7634560906515581\n","\n","\n","Epoch 5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Training loss 0.7630113213081059 Training accuracy 0.8148695224027573\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Development loss 0.7958508992599229 Development accuracy 0.7606232294617564\n","\n","\n","Epoch 6\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Training loss 0.7333283887252094 Training accuracy 0.8365337272279666\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Development loss 0.8078296086882467 Development accuracy 0.7818696883852692\n","\n","\n","Epoch 7\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Training loss 0.7164710872168616 Training accuracy 0.8535204332840965\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Development loss 0.7982070011607671 Development accuracy 0.774787535410765\n","\n","\n","Epoch 8\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Training loss 0.7156666988345581 Training accuracy 0.8552437223042837\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Development loss 0.7873816139953959 Development accuracy 0.7762039660056658\n","\n","\n"]}],"source":["#fine tuning ROBERTa and validating it \n","\n","for epoch in range(EPOCHS):\n","    print(f'Epoch {epoch + 1}')\n","    trainLoss, trainAccuracy = train_epoch(\n","        model,\n","        trainDataLoader,\n","        lossFunction,\n","        optimizer,\n","        device,\n","        scheduler,\n","        len(trainDf)\n","      )\n","    print(f'Training loss {trainLoss} Training accuracy {trainAccuracy}')\n","    devLoss, devAccuracy = eval_model(\n","        model,\n","        developmentDataLoader,\n","        lossFunction,\n","        device,\n","        len(devDf)\n","      )\n","    print(f'Development loss {devLoss} Development accuracy {devAccuracy}')\n","    # from google.colab import drive\n","    # drive.mount('/content/gdrive')\n","    \n","    # model_save_name = f'Roberta_tfidf_{epoch}.pt'\n","    # path = F\"/content/gdrive/My Drive/{model_save_name}\" \n","    # torch.save(model.state_dict(), path)\n","    print()\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3cvsigzislBE"},"outputs":[],"source":["#This function gets the predictions from the model after it is trained.\n","def get_predictions(model, data_loader):\n","\n","    model = model.eval()\n","    review_texta = []\n","#     review_textb = []               #     !! Change - commented\n","    predictions = []\n","    prediction_probs = []\n","    real_values = []\n","\n","    with torch.no_grad():\n","        for d in data_loader:\n","\n","            textSrcInre                 = d[\"TextSrcInre\"]\n","#             textbs                 = d[\"secondSeq\"]\n","            input_ids              = d[\"input_ids\"].to(device)\n","            attention_mask         = d[\"attention_mask\"].to(device)\n","            labels                 = d[\"labelValue\"].to(device)\n","            Features            = d[\"Features\"]\n","            tfidf_transform        = tfidf.transform(textSrcInre)\n","            tfidf_transform_tensor = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_transform)).float()\n","\n","            pca_tensor             =  p.transform(tfidf_transform_tensor)\n","\n","            pca_tensor = torch.from_numpy(pca_tensor).float()\n","            pca_tensor = pca_tensor.to(device)\n","            tfidf_transform_tensor = tfidf_transform_tensor.to(device)\n","\n","            #Getting the softmax output from model\n","            outputs = model(\n","                input_ids             = input_ids,\n","                attention_mask        = attention_mask,\n","                inputs_tfidf_feats    = tfidf_transform_tensor,\n","                pca_transformed_feats = pca_tensor,\n","                modelType             = 'roberta-TrainedTfIdf'\n","                )\n","            _, preds = torch.max(outputs, dim=1)     #Determining the model predictions\n","\n","            review_texta.extend(textSrcInre)\n","#             review_textb.extend(textbs)\n","            predictions.extend(preds)\n","            prediction_probs.extend(outputs)\n","            real_values.extend(labels)\n","    predictions = torch.stack(predictions).cpu()\n","    prediction_probs = torch.stack(prediction_probs).cpu()\n","    real_values = torch.stack(real_values).cpu()\n","  \n","    return review_texta, predictions, prediction_probs, real_values\n","#    return review_texta, review_textb, predictions, prediction_probs, real_values\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":28914,"status":"ok","timestamp":1649607183343,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"},"user_tz":-120},"id":"0eCWXLuvslTG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0345d445-fb0b-4c00-b2da-c9d5dcc38b1b"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}],"source":["#Getting model predictions on dev dataset\n","# firstSeq_dev, secondSeq_dev, yHat_dev, predProbs_dev, yTest_dev = get_predictions(\n","#   model,\n","#   developmentDataLoader\n","# )\n","\n","firstSeq_dev, yHat_dev, predProbs_dev, yTest_dev = get_predictions(\n","  model,\n","  developmentDataLoader\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1649607183344,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"},"user_tz":-120},"id":"kOufuWqIslba","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b009ac0b-0ad5-4a4d-f209-30b513a4d723"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","  MajorClaim     0.7213    0.9072    0.8037        97\n","       Claim     0.5607    0.5543    0.5575       175\n","     Premise     0.8832    0.8364    0.8592       434\n","\n","    accuracy                         0.7762       706\n","   macro avg     0.7217    0.7660    0.7401       706\n","weighted avg     0.7810    0.7762    0.7768       706\n","\n","[[ 88   9   0]\n"," [ 30  97  48]\n"," [  4  67 363]]\n"]}],"source":[" #Printing classification report for dev dataset (Evaluating the model on Dev set)\n","print(classification_report(yTest_dev, yHat_dev, target_names= CLASS_NAMES,digits=4))\n","print(confusion_matrix(yTest_dev, yHat_dev))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kbfBps65st6u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649607213034,"user_tz":-120,"elapsed":29700,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"outputId":"c59d66c6-5d73-420f-c8e8-e2d095cb1bd6"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}],"source":["#Getting model predictions on test dataset\n","firstSeq_test, yHat_test, predProbs_test, yTest_test = get_predictions(\n","  model,\n","  testDataLoader\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECQy76TAst_i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649607213036,"user_tz":-120,"elapsed":50,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"outputId":"06760f7a-52c4-41d8-e177-efb2442451bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","  MajorClaim     0.6875    0.9625    0.8021        80\n","       Claim     0.5870    0.5684    0.5775       190\n","     Premise     0.8703    0.8200    0.8444       450\n","\n","    accuracy                         0.7694       720\n","   macro avg     0.7149    0.7836    0.7413       720\n","weighted avg     0.7752    0.7694    0.7693       720\n","\n"]}],"source":["#Printing classification report for test dataset (Evaluating the model on test set)\n","print(classification_report(yTest_test, yHat_test, target_names= CLASS_NAMES,digits=4))"]},{"cell_type":"code","source":["print(confusion_matrix(yTest_test, yHat_test))"],"metadata":{"id":"sUnTHP_lOHcK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649607213039,"user_tz":-120,"elapsed":46,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"outputId":"167bf94b-3873-435a-d7f6-5954251803bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 77   3   0]\n"," [ 27 108  55]\n"," [  8  73 369]]\n"]}]},{"cell_type":"markdown","source":["**Prediction Starts from here**"],"metadata":{"id":"zKUti5YbjvW8"}},{"cell_type":"code","source":["#Creates a dataset which will be used to feed to RoBERTa\n","class StanceIcleDataset(data.Dataset):\n","  def __init__(self,TextSrcInre,  tokenizer, max_len):\n","\n","    # def __init__(self,firstSeq,TextSrcInre, labelValue,  tokenizer, max_len):\n","#     def __init__(self, firstSeq, secondSeq, TextSrcInre, labelValue,  tokenizer, max_len):\n","        # self.firstSeq    = firstSeq      #First input sequence that will be supplied to RoBERTa\n","        # self.secondSeq   = secondSeq     #Second input sequence that will be supplied to RoBERTa\n","        self.TextSrcInre = TextSrcInre   #Concatenation of reply+ previous+ src text to get features from 1 training example\n","        self.tokenizer   = tokenizer     #tokenizer that will be used to tokenize input sequences (Uses BERT-tokenizer here)\n","        self.max_len     = max_len       #Maximum length of the tokens from the input sequence that BERT needs to attend to\n","\n","  def __len__(self):\n","        return len(self.TextSrcInre)\n","\n","  def __getitem__(self, item):\n","        # firstSeq    = str(self.firstSeq[item])\n","        # secondSeq   = str(self.secondSeq[item])\n","        TextSrcInre = str(self.TextSrcInre[item])\n","\n","    #Encoding the first and the second sequence to a form accepted by RoBERTa\n","    #RoBERTa does not use token_type_ids to distinguish the first sequence from the second sequnece.\n","        encoding = tokenizer.encode_plus(\n","            # firstSeq,\n","            # secondSeq,\n","            TextSrcInre,\n","            max_length = self.max_len,\n","            add_special_tokens= True,\n","            truncation = True,\n","            pad_to_max_length = True,\n","            # padding=True,\n","            return_attention_mask = True,\n","            return_tensors = 'pt'\n","        )\n","\n","        return {\n","            # 'firstSeq' : firstSeq,\n","            # 'secondSeq' : secondSeq,\n","            'TextSrcInre': TextSrcInre,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","        }\n"],"metadata":{"id":"aHvCYJvJkDPS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Creates a data loader\n","def createIcleDataLoader(dataframe, tokenizer, max_len, batch_size):\n","    ds = StanceIcleDataset(\n","        TextSrcInre = dataframe.Text.to_numpy(),\n","        # Features = dataframe.Features.to_numpy(),\n","        tokenizer   = tokenizer,\n","        max_len     = max_len\n","    )\n","\n","    return data.DataLoader(\n","        ds,\n","        batch_size  = batch_size,\n","        shuffle     = False,\n","        num_workers = 2\n","    )"],"metadata":{"id":"xmeWZGXZvn5X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#This function gets the predictions from the model after it is trained.\n","def get_icle_predictions(model, data_loader):\n","\n","    model = model.eval()\n","    review_texta = []\n","#     review_textb = []               #     !! Change - commented\n","    predictions = []\n","    prediction_probs = []\n","    real_values = []\n","\n","    with torch.no_grad():\n","        for d in data_loader:\n","\n","            textSrcInre                 = d[\"TextSrcInre\"]\n","#             textbs                 = d[\"secondSeq\"]\n","            input_ids              = d[\"input_ids\"].to(device)\n","            attention_mask         = d[\"attention_mask\"].to(device)\n","            # labels                 = d[\"labelValue\"].to(device)\n","            # Features            = d[\"Features\"]\n","            tfidf_transform        = tfidf.transform(textSrcInre)\n","            tfidf_transform_tensor = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_transform)).float()\n","\n","            pca_tensor             =  p.transform(tfidf_transform_tensor)\n","\n","            pca_tensor = torch.from_numpy(pca_tensor).float()\n","            pca_tensor = pca_tensor.to(device)\n","            tfidf_transform_tensor = tfidf_transform_tensor.to(device)\n","\n","            #Getting the softmax output from model\n","            outputs = model(\n","                input_ids             = input_ids,\n","                attention_mask        = attention_mask,\n","                inputs_tfidf_feats    = tfidf_transform_tensor,\n","                pca_transformed_feats = pca_tensor,\n","                modelType             = 'roberta-TrainedTfIdf'\n","                )\n","            _, preds = torch.max(outputs, dim=1)     #Determining the model predictions\n","\n","            review_texta.extend(textSrcInre)\n","#             review_textb.extend(textbs)\n","            predictions.extend(preds)\n","            prediction_probs.extend(outputs)\n","            # real_values.extend(labels)\n","    predictions = torch.stack(predictions).cpu()\n","    prediction_probs = torch.stack(prediction_probs).cpu()\n","    # real_values = torch.stack(real_values).cpu()\n","  \n","    return review_texta, predictions, prediction_probs\n","#    return review_texta, review_textb, predictions, prediction_probs, real_values\n"],"metadata":{"id":"u18E6iRemI-Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def int_to_label(label):\n","    if label   == 0:\n","        return 'MajorClaim'\n","    elif label == 1:\n","        return 'Claim'\n","    elif label == 2:\n","        return 'Premise'"],"metadata":{"id":"WteQ1JYimLVS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Creating data loader for development data\n","path3= F\"/content/gdrive/My Drive/Colab Notebooks/1. ADU_Classification/ICLE1000_ADU_NonADU_epoch5_roberta_large.xlsx\" \n","icle_df=pd.read_excel(path3,sheet_name='Sheet1',usecols=['Text','Ann_Adu'])\n","icle_adu_df=icle_df.loc[icle_df['Ann_Adu']!='Non-ADU']\n","icle_adu_df['index1'] = icle_adu_df.index\n","ICLE_DataLoader  = createIcleDataLoader(icle_adu_df, tokenizer, MAX_LENGTH, BATCH_SIZE)"],"metadata":{"id":"yp2XeKPmmNle","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645650115533,"user_tz":-60,"elapsed":2738,"user":{"displayName":"Garima Mudgal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihbN2FEuxMGO0JqJ3fdQhmaS3HMx15Ipgmo6tbPQ=s64","userId":"14901317976461361357"}},"outputId":"0fa0196d-72fc-494e-8359-afa7c0c1952a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"\n"]}]},{"cell_type":"code","source":["icle_adu_df.Ann_Adu.value_counts()"],"metadata":{"id":"a2qnwKlz22Hu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645650115534,"user_tz":-60,"elapsed":18,"user":{"displayName":"Garima Mudgal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihbN2FEuxMGO0JqJ3fdQhmaS3HMx15Ipgmo6tbPQ=s64","userId":"14901317976461361357"}},"outputId":"d756ba40-67b6-4e00-e9b2-f470dc23c098"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ADU    26781\n","Name: Ann_Adu, dtype: int64"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["firstSeq_ICLE, yHat_icle, predProbs_icle = get_icle_predictions(\n","  model,\n","  ICLE_DataLoader\n",")"],"metadata":{"id":"KqYmlcm4nFBV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645650737354,"user_tz":-60,"elapsed":621835,"user":{"displayName":"Garima Mudgal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihbN2FEuxMGO0JqJ3fdQhmaS3HMx15Ipgmo6tbPQ=s64","userId":"14901317976461361357"}},"outputId":"76060d12-0cae-4933-99f6-c51a475fb298"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["# from google.colab import files \n","\n","index_1=pd.DataFrame(icle_adu_df['index1'])\n","index_1=index_1.reset_index(drop=True)\n","predictions=pd.DataFrame(list(zip(firstSeq_ICLE, yHat_icle.numpy())),columns=['Text','ADU_Type'])\n","concatenated_dataframes = pd.concat([index_1, predictions],axis=1,ignore_index=True)\n","concatenated_dataframes.columns=['index','Text','Ann_Adu']\n","concatenated_dataframes['Ann_Adu']=concatenated_dataframes.Ann_Adu.apply(int_to_label)\n","concatenated_dataframes.Ann_Adu.value_counts()"],"metadata":{"id":"mcmd2f3mnS2s","executionInfo":{"status":"ok","timestamp":1645650737357,"user_tz":-60,"elapsed":58,"user":{"displayName":"Garima Mudgal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihbN2FEuxMGO0JqJ3fdQhmaS3HMx15Ipgmo6tbPQ=s64","userId":"14901317976461361357"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"20a031f6-d759-4f98-f7e7-a76902c295d0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Premise       21332\n","Claim          5249\n","MajorClaim      200\n","Name: Ann_Adu, dtype: int64"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["# icle_non_adu_df=icle_df.loc[icle_df['Ann_Adu']=='Non-ADU']\n","# icle_non_adu_df=icle_non_adu_df.reset_index()\n","# icle_non_adu_df['Ann_Adu']='None'\n","# icle_non_adu_df=icle_non_adu_df.reset_index(drop=True)\n","# icle_non_adu_df"],"metadata":{"id":"aS8MO9yM4WHY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# result=concatenated_dataframes.append(icle_non_adu_df, ignore_index=True)\n","# result=result.sort_values(by='index')\n","# result"],"metadata":{"id":"7doq13b14c7U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# result.Ann_Adu.value_counts()"],"metadata":{"id":"67y9od4N1Nw6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# result.to_excel('Final_ADU_ICLE_5085.xlsx')"],"metadata":{"id":"WorhQTYnnmCX"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"31.03.2022Step2_Roberta_TFIDF_Attempt3_CMV+AAE_ADU_Classification.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c1ce079b2c144c179386631e3253ef3c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1022484f52c248e09a2944b43f1f9c7d","IPY_MODEL_f8d24ddf39474a18b022cbcbd3234648","IPY_MODEL_70f53776b8b14385a2a293929f2118f1"],"layout":"IPY_MODEL_847a31ae51454fa58bb8dea044ddf0a1"}},"1022484f52c248e09a2944b43f1f9c7d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9298608d59345efa27c48f52522fe4e","placeholder":"​","style":"IPY_MODEL_99d6706642b242a691162c6809063179","value":"Downloading: 100%"}},"f8d24ddf39474a18b022cbcbd3234648":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9139b5e5f69847e2a336ebad5ce1f5fc","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5016ae1ba9dd4eccb74426e444352e6a","value":898823}},"70f53776b8b14385a2a293929f2118f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f432e08e9964e56834ab9e597820eeb","placeholder":"​","style":"IPY_MODEL_bb876e0dc8744d87bd1eab77fb01ec19","value":" 878k/878k [00:00&lt;00:00, 2.81MB/s]"}},"847a31ae51454fa58bb8dea044ddf0a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9298608d59345efa27c48f52522fe4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99d6706642b242a691162c6809063179":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9139b5e5f69847e2a336ebad5ce1f5fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5016ae1ba9dd4eccb74426e444352e6a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3f432e08e9964e56834ab9e597820eeb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb876e0dc8744d87bd1eab77fb01ec19":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"028e97bea1bd41b7801fb06754c4ce2b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d53a5d619c3e4192822a5176a1addc36","IPY_MODEL_717c2879940449eb9544daa07b909a09","IPY_MODEL_698efa6100e0454b87b806dc558637f7"],"layout":"IPY_MODEL_e3e8e9bd90da4e3c9a4f7850c3567dcf"}},"d53a5d619c3e4192822a5176a1addc36":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_031d45323e61420eaf10ec13747323d6","placeholder":"​","style":"IPY_MODEL_c2fe4181ff3548a5a8895775329ec48e","value":"Downloading: 100%"}},"717c2879940449eb9544daa07b909a09":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a5799690fb04647a2ed6ab6e35a2bad","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6c19b3611f3b478cb52bd48424925adc","value":456318}},"698efa6100e0454b87b806dc558637f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_174b541734bd448aa83b5059cef6aefc","placeholder":"​","style":"IPY_MODEL_e814678c4f3a4285b3249a165294ca23","value":" 446k/446k [00:00&lt;00:00, 931kB/s]"}},"e3e8e9bd90da4e3c9a4f7850c3567dcf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"031d45323e61420eaf10ec13747323d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2fe4181ff3548a5a8895775329ec48e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a5799690fb04647a2ed6ab6e35a2bad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c19b3611f3b478cb52bd48424925adc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"174b541734bd448aa83b5059cef6aefc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e814678c4f3a4285b3249a165294ca23":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0406697340ad4a4ea15620c6eda51412":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_47dbc817b6f34efcb1b6d63987bdf7c6","IPY_MODEL_fd1e0a822cf34864b5e592ce7447f80c","IPY_MODEL_f0ef4786f57a4bcaaf170630dd24c5b1"],"layout":"IPY_MODEL_815a8de03dc74718a985f8f1fd1bd858"}},"47dbc817b6f34efcb1b6d63987bdf7c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e3ff28f3e6544c7a9b9805358ea6118","placeholder":"​","style":"IPY_MODEL_21dd5e07391a419c8185aa109506ce8b","value":"Downloading: 100%"}},"fd1e0a822cf34864b5e592ce7447f80c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e44f886a64834c7bbbd2428f042983ed","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e07d46edf1604929a977ac1353f330e2","value":482}},"f0ef4786f57a4bcaaf170630dd24c5b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a9e63c22c414c33a1ad9e5aea1bd6ba","placeholder":"​","style":"IPY_MODEL_dc8ecbdcbe0349048ae2ee3239a1ef38","value":" 482/482 [00:00&lt;00:00, 15.6kB/s]"}},"815a8de03dc74718a985f8f1fd1bd858":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e3ff28f3e6544c7a9b9805358ea6118":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21dd5e07391a419c8185aa109506ce8b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e44f886a64834c7bbbd2428f042983ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e07d46edf1604929a977ac1353f330e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a9e63c22c414c33a1ad9e5aea1bd6ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc8ecbdcbe0349048ae2ee3239a1ef38":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5287facbaed74d6e8e802a2acbded356":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a4712244534b4f219b38267ba2960efd","IPY_MODEL_47fe510231284bdf84bdffbcd1545ca5","IPY_MODEL_e73301c7aedb4fc3851041fc1cf874c9"],"layout":"IPY_MODEL_e2383f6ba5494b7b91753b741b36043b"}},"a4712244534b4f219b38267ba2960efd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c64363a260824bd9b8c1cef74f698bd6","placeholder":"​","style":"IPY_MODEL_d49d4477b420422abad96f6cef55e062","value":"Downloading: 100%"}},"47fe510231284bdf84bdffbcd1545ca5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0341f6aca88348ff8593ca8fe30226bb","max":1425941629,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1b456e4165e54f2b81b1b40d235a03a6","value":1425941629}},"e73301c7aedb4fc3851041fc1cf874c9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0d5fa872b4f4b9fa0626248ab85431e","placeholder":"​","style":"IPY_MODEL_90f07865d677456d8205e8ae6755f142","value":" 1.33G/1.33G [01:32&lt;00:00, 17.5MB/s]"}},"e2383f6ba5494b7b91753b741b36043b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c64363a260824bd9b8c1cef74f698bd6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d49d4477b420422abad96f6cef55e062":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0341f6aca88348ff8593ca8fe30226bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b456e4165e54f2b81b1b40d235a03a6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c0d5fa872b4f4b9fa0626248ab85431e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90f07865d677456d8205e8ae6755f142":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}