{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6379,"status":"ok","timestamp":1648889676897,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"},"user_tz":-120},"id":"5tGzncstOkkX"},"outputs":[],"source":["import re\n","import scipy\n","import pandas         as pd\n","import io\n","import numpy          as np\n","import copy\n","\n","import torch\n","\n","from sklearn.metrics                  import classification_report\n","from sklearn.feature_extraction.text  import TfidfVectorizer\n","from sklearn.utils                    import class_weight\n","\n","from sklearn.model_selection import train_test_split\n","from torch                            import nn, optim\n","from torch.utils                      import data"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1648889676898,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"},"user_tz":-120},"id":"_xpbMF9BOsLd"},"outputs":[],"source":["#Seeding for deterministic results\n","RANDOM_SEED = 16\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","HIDDEN_LAYER_UNITS = 128\n","\n","# CLASS_NAMES = ['support', 'deny', 'query', 'comment']\n","# CLASS_NAMES = ['None','MajorClaim','Claim','Premise']\n","CLASS_NAMES =['Non-ADU','ADU']\n","\n","EPOCHS      = 50"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1648889676898,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"},"user_tz":-120},"id":"322vFZJMOuo0"},"outputs":[],"source":["#Converting labels to numbers\n","def label_to_int(label):\n","    if label   == 0:\n","        return 0\n","    elif label == 1:\n","        return 1\n","    elif label == 2:\n","        return 1\n","    elif label == 3:\n","        return 1"]},{"cell_type":"code","source":["#Converting labels to numbers\n","def adu_nonadu(label):\n","    if label   == 'None':\n","        return 'Non-ADU'\n","    elif label == 'MajorClaim':\n","        return 'ADU'\n","    elif label == 'Claim':\n","        return  'ADU'\n","    elif label == 'Premise':\n","        return  'ADU'"],"metadata":{"id":"hV0EiqZbJUq_","executionInfo":{"status":"ok","timestamp":1648889676899,"user_tz":-120,"elapsed":10,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1648889676899,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"},"user_tz":-120},"id":"8w-D2dBwOwi0"},"outputs":[],"source":["def processStanceData(df):\n","    result1  = df.replace(np.nan, '', regex=True)                               #Getting rid of NaN values\n","\n","    result1['labelValue'] = result1.ADU_Type.apply(label_to_int)       \n","    result1['ADU_Type'] = result1.ADU_Type.apply(adu_nonadu)                 #Converting labels to numbers\n","    result1['TextSrcInre']    = result1['Text']\n","    result1['Features']    = result1['Sentence_Label'].str.cat(result1['Paragraph_Label'],sep=\" , \") \n","    data = result1[['Text','Topic','TextSrcInre','Para_No','ADU_Type','labelValue','Features']].copy()    \n","    data.columns = ['Text','Topic','TextSrcInre','Para_No','ADU_Type','labelValue','Features']\n","    return data\n","\n","def processStanceData_cmv(df):\n","    result1  = df.replace(np.nan, '', regex=True)                               #Getting rid of NaN values\n","\n","    result1['labelValue'] = result1.ADU_Type.apply(label_to_int)       \n","    result1['ADU_Type'] = result1.ADU_Type.apply(adu_nonadu)                 #Converting labels to numbers\n","    result1['TextSrcInre']    = result1['Text']\n","    result1['Features']    = result1['Sentence_Label'] \n","    result1['Para_No']    = ''\n","    data = result1[['Text','Topic','TextSrcInre','Para_No','ADU_Type','labelValue','Features']].copy()    \n","    data.columns = ['Text','Topic','TextSrcInre','Para_No','ADU_Type','labelValue','Features']\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7c9BLPkOwoN"},"outputs":[],"source":["# Reading data from AAE premise and claims file as dataFrames\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","path = F\"/content/gdrive/My Drive/Colab Notebooks/1. ADU_Classification/Data/\" "]},{"cell_type":"code","source":["# trainDf=pd.read_csv(path+'model1_train_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","# trainDf=trainDf.drop(columns='Unnamed: 0')\n","# devDf=pd.read_csv(path+'model1_dev_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","# devDf=devDf.drop(columns='Unnamed: 0')\n","# testDf=pd.read_csv(path+'model1_test_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","# testDf=testDf.drop(columns='Unnamed: 0')\n","\n","# trainDf.labelValue=trainDf.labelValue.apply(label_to_int)\n","# devDf.labelValue=devDf.labelValue.apply(label_to_int)\n","# testDf.labelValue=testDf.labelValue.apply(label_to_int)\n","\n","###--- MODEL 1 modified\n","trainDf=pd.read_csv(path+'model1_train_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","trainDf=trainDf.drop(columns='Unnamed: 0')\n","devDf=pd.read_csv(path+'model1_dev_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","devDf=devDf.drop(columns='Unnamed: 0')\n","testDf=pd.read_csv(path+'model_2_3_testDf_aae.csv',sep=\"\\t\",index_col=False)\n","testDf=testDf.drop(columns='Unnamed: 0')\n","\n","trainDf.labelValue=trainDf.labelValue.apply(label_to_int)\n","devDf.labelValue=devDf.labelValue.apply(label_to_int)\n","testDf.labelValue=testDf.labelValue.apply(label_to_int)\n","\n","\n","\n","# ### ---------- MODEL 2\n","\n","# trainDf=pd.read_csv(path+'model_2_train_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","# trainDf=trainDf.drop(columns='Unnamed: 0')\n","# devDf=pd.read_csv(path+'model_2_dev_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","# devDf=devDf.drop(columns='Unnamed: 0')\n","# testDf=pd.read_csv(path+'model_2_3_testDf_aae.csv',sep=\"\\t\",index_col=False)\n","# testDf=testDf.drop(columns='Unnamed: 0')\n","\n","# trainDf.labelValue=trainDf.labelValue.apply(label_to_int)\n","# devDf.labelValue=devDf.labelValue.apply(label_to_int)\n","# testDf.labelValue=testDf.labelValue.apply(label_to_int)\n","\n","### ---------- MODEL 3\n","\n","# trainDf=pd.read_csv(path+'model3_trainDf_aae.csv',sep=\"\\t\",index_col=False)\n","# trainDf=trainDf.drop(columns='Unnamed: 0')\n","# devDf=pd.read_csv(path+'model3_devDf_aae.csv',sep=\"\\t\",index_col=False)\n","# devDf=devDf.drop(columns='Unnamed: 0')\n","# testDf=pd.read_csv(path+'model_2_3_testDf_aae.csv',sep=\"\\t\",index_col=False)\n","# testDf=testDf.drop(columns='Unnamed: 0')\n","\n","# trainDf.labelValue=trainDf.labelValue.apply(label_to_int)\n","# devDf.labelValue=devDf.labelValue.apply(label_to_int)\n","# testDf.labelValue=testDf.labelValue.apply(label_to_int)"],"metadata":{"id":"U8fSW7G_wl9o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train=trainDf.labelValue"],"metadata":{"id":"B4YVoiZEy03h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(trainDf), len(devDf),len(testDf))\n","print((trainDf.labelValue.value_counts()), (devDf.labelValue.value_counts()),(testDf.labelValue.value_counts()))"],"metadata":{"id":"xdkC1jTSyNxZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainDf.head()"],"metadata":{"id":"7czlePnfygzL"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YmR92HCfOwrR"},"outputs":[],"source":["x_train = trainDf['TextSrcInre'].tolist()\n","y_train = trainDf['labelValue'].tolist()\n","x_dev  = devDf['TextSrcInre'].tolist()\n","y_dev  = devDf['labelValue'].tolist()\n","x_test = testDf['TextSrcInre'].tolist()\n","y_test = testDf['labelValue'].tolist()\n","\n","#Instantiating TfidfVectorizer object and fitting it on the training set\n","tfidf = TfidfVectorizer(min_df = 10, max_df = 0.5, ngram_range=(1,2))\n","print(tfidf)\n","x_train_feats = tfidf.fit(x_train)\n","print('x_train_feats: ',x_train_feats)\n","print('length: ',len(x_train_feats.get_feature_names()))\n","\n","x_train_transform = x_train_feats.transform(x_train)\n","#Converting the TF-IDF matrix to tensor\n","tfidf_transform_tensor = torch.tensor(scipy.sparse.csr_matrix.todense(x_train_transform)).float()\n","print('x_train_transform.shape: ',x_train_transform.shape)\n","\n","#Tranforming the development and test data to tf-idf matrix\n","x_dev  = tfidf.transform(x_dev)\n","x_test = tfidf.transform(x_test)\n","\n","x_dev  = torch.tensor(scipy.sparse.csr_matrix.todense(x_dev)).float()\n","x_test = torch.tensor(scipy.sparse.csr_matrix.todense(x_test)).float()"]},{"cell_type":"code","source":["class_weights = class_weight.compute_class_weight(\n","                                        class_weight = \"balanced\",\n","                                        classes = np.unique(y_train),\n","                                        y = y_train                                                   \n","                                    )\n","class_weights"],"metadata":{"id":"Jn9cNPvky-lk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7iXaDblzOwuF"},"outputs":[],"source":["#Converting prections for train, dev and test data to tensors\n","y_train = torch.tensor(y_train)\n","y_dev   = torch.tensor(y_dev)\n","y_test  = torch.tensor(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4QZ9GogjOwwi"},"outputs":[],"source":["class Tfidf_Nn(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        # Inputs to hidden layer linear transformation\n","        self.hidden  = nn.Linear(len(tfidf.get_feature_names()), HIDDEN_LAYER_UNITS)\n","        # Output layer\n","        self.output  =  nn.Linear(HIDDEN_LAYER_UNITS, len(CLASS_NAMES))\n","        self.dropout = nn.Dropout(0.1)\n","        \n","        # Defining tanh activation and softmax output \n","        self.tanh    = nn.Tanh()                                     #Using tanh as it performed better than ReLu during hyper-param optimisation\n","        self.softmax = nn.Softmax(dim=1)\n","        \n","    def forward(self, x):\n","        # Pass the input tensor through each of the below operations\n","        x = self.hidden(x)\n","        #print(x.shape)\n","        y = self.tanh(x)\n","        #print(y.shape)\n","        z = self.dropout(y)\n","        #print(z.shape)\n","        z = self.output(z)\n","        #print(z.shape)\n","        z = self.softmax(z)\n","        \n","        #returning the output from hidden layer and the output layer\n","        return  y, z\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQrOlsuIOw2Z"},"outputs":[],"source":["#Defining the model\n","model = Tfidf_Nn()\n","\n","# Defining the loss\n","'''Using class-weights to accomodate heavily imbalanced data. \n","These weights were learnt by running several experiments using \n","other weights and the weights that produced the best results have\n"," finally been used here'''\n","\n","weights       = class_weights\n","class_weights = torch.FloatTensor(weights)\n","criterion     = nn.CrossEntropyLoss(weight = class_weights)\n","\n","\n","# Forward pass, get our logits\n","hidden_state_output, classfier_output = model(tfidf_transform_tensor)\n","print(classfier_output)\n","print(classfier_output[0].shape)\n","\n","loss = criterion(classfier_output, y_train)\n","\n","loss.backward()\n","\n","# Optimizers require the parameters to optimize and a learning rate\n","optimizer = optim.Adam(model.parameters(), lr=0.02)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XfjFcSLUOw7N"},"outputs":[],"source":["#Training the model on training data and evaluating it on development set\n","#%%time\n","def train_model():\n","    train_losses = []\n","    dev_losses = []\n","    dev_accuracies = []\n","\n","    for e in range(EPOCHS):\n","        correct_predictions = 0\n","        optimizer.zero_grad()\n","\n","        hidden_layer_output, classifier_output = model.forward(tfidf_transform_tensor)\n","\n","        loss = criterion(classifier_output, y_train)\n","        loss.backward()\n","        train_loss = loss.item()\n","        train_losses.append(train_loss)\n","\n","        optimizer.step()\n","        with torch.no_grad():\n","            model.eval()\n","\n","            #Getting hidden layer and softmax output from model for dev data\n","            hidden_layer_output, classifier_output = model(x_dev)\n","\n","            #Calculating loss\n","            dev_loss = criterion(classifier_output, y_dev)\n","            dev_losses.append(dev_loss)\n","\n","            #Calculating values predicted by the model\n","            _, preds = torch.max(classifier_output, dim=1)\n","            correct_predictions += torch.sum(preds == y_dev)\n","\n","            #Calculating accuracy\n","            dev_accuracy = correct_predictions.double() / len(y_dev)\n","            dev_accuracies.append(dev_accuracy)\n","\n","        model.train()\n","\n","        print(f\"Epoch: {e+1}/{EPOCHS}.. \",\n","              f\"Training Loss: {dev_loss:.3f}.. \",\n","              f\"Dev Loss: {dev_loss:.3f}.. \",\n","              f\"Dev Accuracy: {dev_accuracy:.3f}\")\n","\n","\n","train_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RzfI9lbbPH4x"},"outputs":[],"source":["'''This function gets the predictions for each data point \n","in the deevelopment and the training set'''\n","\n","def get_predictions(model, x_test, y_test):\n","    predictions = []\n","    prediction_probs = []\n","    real_values = []\n","    with torch.no_grad():\n","        model.eval()\n","        labels = y_test\n","\n","        #Currently, not interested in the hidden layer outputs.\n","        _,classifier_output = model(x_test)\n","\n","        #Not interested in the maximum values, interested with the indices of these max values\n","        _, preds = torch.max(classifier_output, dim=1)\n","\n","        predictions.extend(preds)\n","        prediction_probs.extend(classifier_output)\n","        real_values.extend(labels)\n","    predictions = torch.stack(predictions)\n","\n","    prediction_probs = torch.stack(prediction_probs)\n","    real_values = torch.stack(real_values)\n","    return  predictions, prediction_probs, real_values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TdqswyFuPMmM"},"outputs":[],"source":["#Getting predictions for the development set\n","y_pred_dev, y_pred_probs, y_true_dev = get_predictions(\n","  model,\n","  x_dev, \n","  y_dev\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNhC4l-VPOnH"},"outputs":[],"source":["#Printing the classifictaion report for the Development set\n","print(classification_report(y_true_dev, y_pred_dev ,digits =4, target_names=CLASS_NAMES))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mWF0jBQ1PQpS"},"outputs":[],"source":["#Getting the predictions for the test set\n","y_pred_test, y_pred_probs, y_true_test = get_predictions(\n","  model,\n","  x_test, \n","  y_test\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STefjU1EPQs7"},"outputs":[],"source":["print(classification_report(y_true_test, y_pred_test , digits = 4,  target_names=CLASS_NAMES))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K17_2kZIPU4P"},"outputs":[],"source":["# torch.save(model.state_dict(), f'Mlp_AAE_1682.pt')\n","from datetime import datetime\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","model_save_name = F\"Mlp_step1_model_1_modified.pt\"\n","path = F\"/content/gdrive/My Drive/Colab Notebooks/1. ADU_Classification/{model_save_name}\" \n","torch.save(model.state_dict(), path)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"31.03.2022 Step1_MLP_AAE+CMV_Corpus_ADU-NonAdu.ipynb","provenance":[{"file_id":"1B0guqIYhl4l6OzBbEcwPRKuhQdqR6GDA","timestamp":1648725927806}],"machine_shape":"hm","authorship_tag":"ABX9TyM+flje/sOf8mL6Z5G0zwlA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}