{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6800,"status":"ok","timestamp":1648889986953,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"},"user_tz":-120},"id":"HDhcsX3YjBXc","outputId":"ab5d83bd-79d8-4b86-992c-9c1e2854dd15"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}],"source":[" import tensorflow as tf\n","# Getting GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FD_JYSyvjluf","executionInfo":{"status":"ok","timestamp":1648890002998,"user_tz":-120,"elapsed":16054,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"outputId":"fa2bad56-5084-4cc5-d435-e6eec865460b"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"]}],"source":["import torch\n","# If a GPU is available\n","if torch.cuda.is_available():    \n","    #set device to GPU   \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If no GPU is available\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":772,"referenced_widgets":["674ba2c030ed493c9e4643c11feaba69","c927bf88f30e48cfa8c1625b1a716e0a","1e34216bc43c46e2b57f9e2f05baef84","48c46337776b48d6aebf1f598c10441c","0b076874c3da4cf283c67a4c0821aeaf","7c173a92f32e438fbfb25e03c1f9a8fd","f57d79e32f5845abacc6b7efe343ed74","3d27d2405ea148898341e53521418df1","6cadfb36a61e4ba894143320fdef92a7","6e4f740cb5354a17bf4cc7ba5c716f36","c4cb703c21f349a5b27d6f625b0daa73","0aef75cc478244d88a86e8368b75ea8e","29a15cd1b8364e4e8dbd1e96515e4c3f","0d67ded6e63442deaa03fe00ffd7ebed","c16f19a12ceb45b183ae3c27f5218ec9","5e73a639554644adb04814aecde8087b","929587c2ad044ca29f57098fabbac91e","4374479c68924f1393c0714bee792163","1b1a4027003c4473baab083bfd13d814","1c7d1f1fd58a4ceca29017dd7caae75a","d7712343ea5a4c3da8590029b66b0c3c","e76cc33376b44f5da8740cddf7ff8c9b","0934d86f06dd45978a85be3919b5b1af","36d25d559d874a28b68c02bdb468b47d","c23c3b06f7c14194b51cd63463da3eca","e8079551a42a436ebfdcea7a05d99c26","e6fc2b8b2a534e2fb9181bc8bc983cf4","32791dcd164e4d0ea4276069220859f9","70848cdd13794187851cc638233d6403","160231b694da4a058faca755cd138a40","3377e97c4ec14ec6adf095fcab17c1e9","13f779e5a9ce40e69370402f2f0ab1d9","90276d0ddac2494e9b97e1f0ca6414a7"]},"id":"xciRRSHqjous","executionInfo":{"status":"ok","timestamp":1648890015029,"user_tz":-120,"elapsed":12041,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"outputId":"3752541b-2458-415f-a8b9-9d29f36ffcf4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 4.7 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 61.3 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 5.3 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 38.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 54.0 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"674ba2c030ed493c9e4643c11feaba69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aef75cc478244d88a86e8368b75ea8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0934d86f06dd45978a85be3919b5b1af"}},"metadata":{}}],"source":["#Importing necessary libraries\n","!pip install transformers\n","\n","import re\n","import scipy\n","import pandas as pd\n","import io\n","import numpy as np\n","import copy\n","import seaborn as sns\n","\n","import transformers\n","from transformers import RobertaModel, RobertaTokenizer, AdamW, get_linear_schedule_with_warmup\n","import torch\n","\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.utils import class_weight\n","\n","from torch import nn, optim\n","from torch.utils import data\n","from sklearn.decomposition import PCA\n","\n","#Seeding for deterministic results\n","RANDOM_SEED = 16\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(RANDOM_SEED)\n","    torch.cuda.manual_seed_all(RANDOM_SEED) \n","    torch.backends.cudnn.deterministic = True  \n","    torch.backends.cudnn.benchmark = False\n","\n","# CLASS_NAMES = ['None','MajorClaim','Claim','Premise']\n","CLASS_NAMES =['Non-ADU','ADU']\n","\n","MAX_LENGTH = 100                                    \n","BATCH_SIZE = 4\n","EPOCHS = 7\n","HIDDEN_UNITS = 128\n","\n","tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-large')  #Use roberta-large or roberta-base"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"35f6prNBjvw7"},"outputs":[],"source":["#Converting labels to numbers\n","def label_to_int(label):\n","    if label   == 0:\n","        return 0\n","    elif label == 1:\n","        return 1\n","    elif label == 2:\n","        return 1\n","    elif label == 3:\n","        return 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZV-5ydlkv4u3"},"outputs":[],"source":["#Converting class names to adu and non-adu\n","def adu_nonadu(label):\n","    if label   == 'None':\n","        return 'Non-ADU'\n","    elif label == 'MajorClaim':\n","        return 'ADU'\n","    elif label == 'Claim':\n","        return  'ADU'\n","    elif label == 'Premise':\n","        return  'ADU'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5jFbj3Fjzb6"},"outputs":[],"source":["def processStanceData(df):\n","                                                   #Concatenating twitter and reddit data\n","    result1  = df.replace(np.nan, '', regex=True)                               #Getting rid of NaN values\n","\n","    result1['labelValue'] = result1.ADU_Type.apply(label_to_int)     \n","    result1['ADU_Type'] = result1.ADU_Type.apply(adu_nonadu)                  #Converting labels to numbers\n","    result1['TextSrcInre']    = result1['Text']\n","    result1['Features']    = result1['Sentence_Label'].str.cat(result1['Paragraph_Label'],sep=\" \") # persing 2010 paper\n","    data = result1[['Text','Topic','TextSrcInre','Para_No','ADU_Type','labelValue','Features']].copy()    \n","    data.columns = ['Text','Topic','TextSrcInre','Para_No','ADU_Type','labelValue','Features']\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vk2N3Axpj2ZE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648890034931,"user_tz":-120,"elapsed":19921,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"outputId":"73dc587c-3c95-4bd2-dd85-92d7fd02e441"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["# Reading data from AAE premise and claims file as dataFrames\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","path= F\"/content/gdrive/My Drive/Colab Notebooks/1. ADU_Classification/Data/\" "]},{"cell_type":"code","source":["# trainDf=pd.read_csv(path+'model1_train_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","# trainDf=trainDf.drop(columns='Unnamed: 0')\n","# devDf=pd.read_csv(path+'model1_dev_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","# devDf=devDf.drop(columns='Unnamed: 0')\n","# testDf=pd.read_csv(path+'model1_test_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","# testDf=testDf.drop(columns='Unnamed: 0')\n","\n","# trainDf.labelValue=trainDf.labelValue.apply(label_to_int)\n","# devDf.labelValue=devDf.labelValue.apply(label_to_int)\n","# testDf.labelValue=testDf.labelValue.apply(label_to_int)\n","\n","\n","### ---------- MODEL 1 modified\n","trainDf=pd.read_csv(path+'model1_train_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","trainDf=trainDf.drop(columns='Unnamed: 0')\n","devDf=pd.read_csv(path+'model1_dev_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","devDf=devDf.drop(columns='Unnamed: 0')\n","testDf=pd.read_csv(path+'model_2_3_testDf_aae.csv',sep=\"\\t\",index_col=False)\n","testDf=testDf.drop(columns='Unnamed: 0')\n","\n","trainDf.labelValue=trainDf.labelValue.apply(label_to_int)\n","devDf.labelValue=devDf.labelValue.apply(label_to_int)\n","testDf.labelValue=testDf.labelValue.apply(label_to_int)\n","\n","### ---------- MODEL 2\n","\n","# trainDf=pd.read_csv(path+'model_2_train_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","# trainDf=trainDf.drop(columns='Unnamed: 0')\n","# devDf=pd.read_csv(path+'model_2_dev_CMV+AAE.csv',sep=\"\\t\",index_col=False)\n","# devDf=devDf.drop(columns='Unnamed: 0')\n","# testDf=pd.read_csv(path+'model_2_3_testDf_aae.csv',sep=\"\\t\",index_col=False)\n","# testDf=testDf.drop(columns='Unnamed: 0')\n","\n","# trainDf.labelValue=trainDf.labelValue.apply(label_to_int)\n","# devDf.labelValue=devDf.labelValue.apply(label_to_int)\n","# testDf.labelValue=testDf.labelValue.apply(label_to_int)\n","\n","### ---------- MODEL 3\n","\n","# trainDf=pd.read_csv(path+'model3_trainDf_aae.csv',sep=\"\\t\",index_col=False)\n","# trainDf=trainDf.drop(columns='Unnamed: 0')\n","# devDf=pd.read_csv(path+'model3_devDf_aae.csv',sep=\"\\t\",index_col=False)\n","# devDf=devDf.drop(columns='Unnamed: 0')\n","# testDf=pd.read_csv(path+'model_2_3_testDf_aae.csv',sep=\"\\t\",index_col=False)\n","# testDf=testDf.drop(columns='Unnamed: 0')\n","\n","# trainDf.labelValue=trainDf.labelValue.apply(label_to_int)\n","# devDf.labelValue=devDf.labelValue.apply(label_to_int)\n","# testDf.labelValue=testDf.labelValue.apply(label_to_int)"],"metadata":{"id":"OVGrl5Ar8d1K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train = trainDf['labelValue'].tolist()\n","class_weights = class_weight.compute_class_weight(\n","                                        class_weight = \"balanced\",\n","                                        classes = np.unique(y_train),\n","                                        y = y_train                                                    \n","                                    )\n","\n","class_weights"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fDUzz7Tf9R8R","executionInfo":{"status":"ok","timestamp":1648890035861,"user_tz":-120,"elapsed":13,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"outputId":"2dc074a1-1559-4a85-d03d-d4cc64958d88"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2.87362086, 0.60532432])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxP62q-fyi2y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648890035862,"user_tz":-120,"elapsed":12,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"outputId":"aa30ab42-d8c0-4511-f0d6-43589fd6dcfd"},"outputs":[{"output_type":"stream","name":"stdout","text":["5730 1012 888\n"]}],"source":["print(len(trainDf), len(devDf),len(testDf))"]},{"cell_type":"code","source":["print(trainDf.ADU_Type.value_counts())\n","print(devDf.ADU_Type.value_counts())\n","print(testDf.ADU_Type.value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n4X29Q-5TF69","executionInfo":{"status":"ok","timestamp":1648890035862,"user_tz":-120,"elapsed":9,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"outputId":"991e9d68-5426-44bd-e68b-e76d1b0d2761"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ADU        4733\n","Non-ADU     997\n","Name: ADU_Type, dtype: int64\n","ADU        840\n","Non-ADU    172\n","Name: ADU_Type, dtype: int64\n","ADU        720\n","Non-ADU    168\n","Name: ADU_Type, dtype: int64\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uv12v5mPkC12"},"outputs":[],"source":["#Creates a dataset which will be used to feed to RoBERTa\n","class StanceDataset(data.Dataset):\n","  def __init__(self,TextSrcInre,Features, labelValue,  tokenizer, max_len):\n","\n","        self.TextSrcInre = TextSrcInre   \n","        self.Features = Features\n","        self.labelValue  = labelValue    \n","        self.tokenizer   = tokenizer     \n","        self.max_len     = max_len      \n","\n","  def __len__(self):\n","        return len(self.labelValue)\n","\n","  def __getitem__(self, item):\n","\n","        TextSrcInre = str(self.TextSrcInre[item])\n","        Features = str(self.Features[item])\n","\n","\n","        encoding = tokenizer.encode_plus(\n","            TextSrcInre,\n","            Features,\n","            max_length = self.max_len,\n","            add_special_tokens= True,\n","            truncation = True,\n","            pad_to_max_length = True,\n","            # padding=True,\n","            return_attention_mask = True,\n","            return_tensors = 'pt'\n","        )\n","\n","        return {\n","            'TextSrcInre': TextSrcInre,\n","            'Features' : Features,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labelValue'  : torch.tensor(self.labelValue[item], dtype=torch.long)\n","        }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RlUhuMaekFgW"},"outputs":[],"source":["#Creates a data loader\n","def createDataLoader(dataframe, tokenizer, max_len, batch_size):\n","    ds = StanceDataset(\n","\n","        TextSrcInre = dataframe.TextSrcInre.to_numpy(),\n","        Features = dataframe.Features.to_numpy(),\n","        labelValue  = dataframe.labelValue.to_numpy(),\n","        tokenizer   = tokenizer,\n","        max_len     = max_len\n","    )\n","\n","    return data.DataLoader(\n","        ds,\n","        batch_size  = batch_size,\n","        shuffle     = True,\n","        num_workers = 2\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C_1_5LUykFox"},"outputs":[],"source":["#Creating data loader for training data\n","trainDataLoader        = createDataLoader(trainDf, tokenizer, MAX_LENGTH, BATCH_SIZE)\n","\n","#Creating data loader for development data\n","developmentDataLoader  = createDataLoader(devDf, tokenizer, MAX_LENGTH, BATCH_SIZE)\n","\n","#Creating data loader for test data\n","testDataLoader         = createDataLoader(testDf, tokenizer, MAX_LENGTH, BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6h1tdcs7F_jo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648890036287,"user_tz":-120,"elapsed":11,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"outputId":"b4127d10-5c5f-47e9-ed61-369586c6c1da"},"outputs":[{"output_type":"stream","name":"stdout","text":["1433\n","253\n","222\n"]}],"source":["print(len(trainDataLoader))\n","print(len(developmentDataLoader))\n","print(len(testDataLoader))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_CO-2GC4kFxV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648890041507,"user_tz":-120,"elapsed":5229,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"outputId":"492575f7-e7d0-4c62-b085-187f3f1a9c9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["x_train_feats:  TfidfVectorizer(max_df=0.5, min_df=10, ngram_range=(1, 2))\n","length:  2255\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]},{"output_type":"stream","name":"stdout","text":["x_train_transform.shape:  (5730, 2255)\n","PCA(n_components=128)\n","torch.DoubleTensor\n","torch.Size([5730, 128])\n","tensor([[-0.0486,  0.0472,  0.0394,  ...,  0.0370,  0.0552, -0.0186],\n","        [ 0.0255, -0.0752,  0.0100,  ...,  0.0109, -0.0187,  0.0272],\n","        [ 0.0091, -0.2001,  0.0163,  ..., -0.0071, -0.0624,  0.0087],\n","        ...,\n","        [-0.0468, -0.1236,  0.0096,  ...,  0.0614,  0.0498, -0.0152],\n","        [ 0.0719,  0.0375, -0.0198,  ..., -0.0673,  0.0073,  0.0326],\n","        [-0.0205,  0.0989, -0.0049,  ...,  0.0531, -0.0612, -0.0165]],\n","       dtype=torch.float64)\n"]}],"source":["#Instantiating the tf-idf vectorizer object\n","tfidf = TfidfVectorizer(min_df = 10, max_df = 0.5, ngram_range=(1,2))\n","\n","x_train = trainDf['TextSrcInre'].tolist()\n","x_train_feats = tfidf.fit(x_train)\n","print('x_train_feats: ',x_train_feats)\n","print('length: ',len(x_train_feats.get_feature_names()))\n","\n","\n","x_train_transform = x_train_feats.transform(x_train)\n","tfidf_transform_tensor = torch.tensor(scipy.sparse.csr_matrix.todense(x_train_transform)).float()\n","print('x_train_transform.shape: ',x_train_transform.shape)\n","\n","\n","pca = PCA(n_components=128)\n","p = pca.fit(tfidf_transform_tensor)\n","# print(p.shape)\n","print(p)\n","X = p.transform(tfidf_transform_tensor)\n","# torch.from_numpy(X.values)\n","X = torch.from_numpy(X)\n","# tfidf_transform_tensor_pca = torch.tensor(scipy.sparse.csr_matrix.todense(X)).float()\n","print(X.type())\n","print(X.shape)\n","print(X)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GWFoeVtrkF4u"},"outputs":[],"source":["#This class defines the model that was used to pre-train a SNN on TF-IDF features\n","class Tfidf_Nn(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        # Inputs to hidden layer linear transformation\n","        self.hidden  = nn.Linear(len(tfidf.get_feature_names()), HIDDEN_UNITS)\n","        # Output layer\n","        self.output  =  nn.Linear(HIDDEN_UNITS, 2)\n","        self.dropout = nn.Dropout(0.1)\n","        \n","        # Defining tanh activation and softmax output \n","        self.tanh = nn.Tanh()\n","        self.softmax = nn.Softmax(dim=1)\n","        \n","    def forward(self, x):\n","        # Pass the input tensor through each of our operations\n","        x = self.hidden(x)\n","        #print(x.shape)\n","        y = self.tanh(x)\n","        #print(y.shape)\n","        z = self.dropout(y)\n","        #print(z.shape)\n","        z = self.output(z)\n","        #print(z.shape)\n","        z = self.softmax(z)\n","        \n","        #Returning the ouputs from the hidden layer and the final output layer\n","        return  y, z\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oCZQ0OE2k13Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648890042580,"user_tz":-120,"elapsed":1077,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"outputId":"ff7d5594-d537-485f-9bb3-dda5da3593a2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]},{"output_type":"execute_result","data":{"text/plain":["Tfidf_Nn(\n","  (hidden): Linear(in_features=2255, out_features=128, bias=True)\n","  (output): Linear(in_features=128, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (tanh): Tanh()\n","  (softmax): Softmax(dim=1)\n",")"]},"metadata":{},"execution_count":18}],"source":["snnmodel = Tfidf_Nn()\n","\n","# model_save_name = 'Mlp_Adu_NonAdu_AAE.pt'\n","model_save_name = 'Mlp_step1_model_1_modified.pt'\n","path = F\"/content/gdrive/My Drive/Colab Notebooks/1. ADU_Classification/{model_save_name}\"\n","\n","snnmodel.load_state_dict(torch.load(path))\n","snnmodel.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKNTUy8Hk16J"},"outputs":[],"source":["'''This class defines the model that will be used for \n","training and testing on the dataset.\n","\n","Adapted from huggingFace\n","This RoBERTa model from huggingface outputs the last hidden states\n","and the pooled output by default. Pooled output is the classification \n","token (1st token of the last hidden state) further processed by a Linear\n","layer and a Tanh activation function.\n","\n","The pre-trained RoBERTa model is used as the primary model.\n","This class experiments with RoBERTa and its ensemble with TF-IDF features. \n","roberta-only :            No ensembling. This just fine-tunes the RoBERTa model. \n","                          The pooled output is passed through a linear layer and \n","                          softmax function is finally used for preictions. \n","\n","roberta-tfIdf :           This model conatenates the 1st token of last-hidden layer\n","                          from RoBERTa with TF-IDF features. Various ways of this \n","                          concatenation was experimented (using pooled output instead\n","                          of 1st token of last hidden layer etc)\n","\n","roberta-pcaTfidf :        This model concatenates the pooled output from\n","                          RoBERTa with the PCA transformed vector.\n","\n","roberta-preTrainedTfIdf : This model concatenates the pooled output from\n","                          RoBERTa with the hidden layer output from a pre-trained\n","                          SNN that was trained on TF-IDF features.\n","\n","Used dropout to prevent over-fitting.'''\n","\n","class StanceClassifier(nn.Module):\n","\n","    def __init__(self,  n_classes):\n","        super(StanceClassifier, self).__init__()\n","        self.robertaModel              = RobertaModel.from_pretrained('roberta-large')    #use roberta-large or roberta-base\n","        self.model_TFIDF               = snnmodel                                        #Pre-trained SNN trained with TF-IDF features\n","\n","        self.drop                      = nn.Dropout(p = 0.3)\n","\n","        self.output                    = nn.Linear(self.robertaModel.config.hidden_size, n_classes)\n","\n","        self.input_size_tfidf_only     = self.robertaModel.config.hidden_size + len(tfidf.get_feature_names())\n","        self.input_size_tfidf_pca      = self.robertaModel.config.hidden_size + HIDDEN_UNITS\n","\n","        self.dense                     = nn.Linear( self.input_size_tfidf_only,  self.input_size_tfidf_only)\n","        self.out_proj                  = nn.Linear( self.input_size_tfidf_only, n_classes)\n","        self.out_pca                   = nn.Linear( self.input_size_tfidf_pca, n_classes)\n","\n","        self.input_size_preTrain_tfidf = self.robertaModel.config.hidden_size +  HIDDEN_UNITS \n","        self.out                       = nn.Linear(self.input_size_preTrain_tfidf, n_classes)\n","\n","        self.softmax                   = nn.Softmax(dim = 1)\n","\n","    def forward(self, input_ids, attention_mask, inputs_tfidf_feats, pca_transformed_feats, modelType):\n","        roberta_output     = self.robertaModel(\n","            input_ids      = input_ids,               #Input sequence tokens\n","            attention_mask = attention_mask )         #Mask to avoid performing attention on padding tokens\n","    #print(roberta_output[1].shape)\n","        if modelType   == 'roberta-only':\n","            pooled_output = roberta_output[1]           #Using pooled output\n","            output        = self.drop(pooled_output)\n","            output        = self.output(output)\n","\n","        elif modelType == 'roberta-tfIdf':\n","            soutput = roberta_output[1]#---------        experimenting with pooled output \n","            #soutput = roberta_output[0][:, 0, :]        #taking <s> token (equivalent to [CLS] token in BERT)\n","            x       = torch.cat((soutput, inputs_tfidf_feats) , dim=1)\n","            x       = self.drop(x)\n","            output  = self.out_proj(x)\n","\n","        elif modelType == 'roberta-pcaTfidf':\n","            soutput = roberta_output[1]\n","            x       = torch.cat((soutput, pca_transformed_feats) , dim=1)\n","            x       = self.drop(x)\n","            output  = self.out_pca(x)\n","\n","        elif modelType == 'roberta-TrainedTfIdf':\n","            tfidf_hidddenLayer, tfidf_output = self.model_TFIDF(inputs_tfidf_feats)\n","            #print(tfidf_hidddenLayer.shape)\n","            #print(tfidf_output.shape)\n","\n","          #Conactenating pooled output from RoBERTa with the hidden layer from the pre-trained SNN using TF-IDF features. \n","          #pooled_output = torch.cat((roberta_output[1], tfidf_output) , dim=1)-------- Experimenting with Output of pre-trained SNN \n","            pooled_output = torch.cat((roberta_output[1], tfidf_hidddenLayer) , dim=1)\n","            output        = self.drop(pooled_output)\n","            output        = self.out(output)\n","\n","        return self.softmax(output)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uerPBwcIk1_F","executionInfo":{"status":"ok","timestamp":1648890086932,"user_tz":-120,"elapsed":44359,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["27a3194603264f729f256fa3a0692eb1","c0e5b3289af54e2eb9551d0a81a0642d","0de4a1a2145846b5a8ce5c3f5fcb93ba","fbb23d2d51d4419ca7bbb05e8f892d2b","58b3f9e6bfe04887a7adf2e4ba1abba7","a42afc24f69f4dcb9d275ecc7a36b991","2356f82b8d4c4393850faf666fa84267","a8b5af16468d4275bffb7f6789940d93","1600b6fad2d94e7182a0b699ba395176","baa55523bba04355b2ceea3dd7d26354","fa152919c8444b04a796efc4d8648e74"]},"outputId":"3250b77f-c8d3-458f-b3e0-8aac2bc9ac1b"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27a3194603264f729f256fa3a0692eb1"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}],"source":["#Instantiating a StanceClassifier object as our model and loading the model onto the GPU.\n","model = StanceClassifier(len(CLASS_NAMES))\n","model = model.to(device)\n","#print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tpzDZjisk2E1","executionInfo":{"status":"ok","timestamp":1648890086932,"user_tz":-120,"elapsed":19,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dbaa24a9-e58d-42b5-98c6-ef68a5ca35f6"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}],"source":["'''Using the same optimiser as used in BERT paper\n","with a different learning rate'''\n","optimizer = AdamW(model.parameters(), \n","                  lr = 2e-6, \n","                  # lr = 1e-5,\n","                  correct_bias= False)\n","\n","totalSteps = len(trainDataLoader) * EPOCHS\n","\n","scheduler = get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=0,\n","            num_training_steps = totalSteps\n",")\n","\n","'''Using class-weights to accomodate heavily imbalanced data. \n","These weights were learnt by running several experiments using \n","other weights and the weights that produced the best results have\n","finally been used here'''\n","\n","# weights      = [1.0, 1.0, 1.0, 1.0]\n","weights = class_weights\n","classWeights = torch.FloatTensor(weights)\n","lossFunction = nn.CrossEntropyLoss(weight = classWeights).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xY9qwj8jk2Hg"},"outputs":[],"source":["#This function is used for training the model with 'roberta-TrainedTfIdf'. \n","def train_epoch(\n","  model,\n","  dataLoader,\n","  lossFunction,\n","  optimizer,\n","  device,\n","  scheduler,\n","  n_examples\n","):\n","    model = model.train()\n","    losses = []\n","    correctPredictions = 0\n","\n","    for d in dataLoader:\n","    \n","        input_ids              = d[\"input_ids\"].to(device)                           #Loading input ids to GPU\n","        attention_mask         = d[\"attention_mask\"].to(device)                      #Loading attention mask to GPU\n","        labelValues            = d[\"labelValue\"].to(device)                          #Loading label value to GPU\n","        textSrcInre            = d[\"TextSrcInre\"]\n","        Features               = d[\"Features\"]                                  \n","        tfidf_transform        = x_train_feats.transform(textSrcInre)\n","        tfidf_transform_tensor = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_transform)).float()   \n","        pca_tensor             = p.transform(tfidf_transform_tensor)\n","\n","        pca_tensor = torch.from_numpy(pca_tensor).float()\n","        pca_tensor = pca_tensor.to(device)\n","        tfidf_transform_tensor = tfidf_transform_tensor.to(device)\n","\n","        #Getting the output from our model (Object of StanceClassification class) for train data\n","        outputs = model(\n","          input_ids             = input_ids,\n","          attention_mask        = attention_mask,\n","          inputs_tfidf_feats    = tfidf_transform_tensor,\n","          pca_transformed_feats = pca_tensor,\n","          modelType             = 'roberta-TrainedTfIdf'\n","        )\n","\n","        #Determining the model predictions\n","        _, predictionIndices = torch.max(outputs, dim=1)\n","        loss = lossFunction(outputs, labelValues)\n","\n","        #Calculating the correct predictions for accuracy\n","        correctPredictions += torch.sum(predictionIndices == labelValues)\n","        losses.append(loss.item())\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","    return np.mean(losses), correctPredictions.double() / n_examples\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmKzvuIElHFx"},"outputs":[],"source":["#This function is used for evaluating the model on the development and test set\n","def eval_model(\n","    model, \n","    dataLoader, \n","    lossFunction,\n","    device,\n","    n_examples\n","    ):\n","    model = model.eval()\n","    losses = []\n","    correctPredictions = 0\n","\n","    with torch.no_grad():\n","        for d in dataLoader:\n","            input_ids              = d[\"input_ids\"].to(device)                          #Loading input ids to GPU\n","            attention_mask         = d[\"attention_mask\"].to(device)                     #Loading attention mask to GPU\n","            labelValues            = d[\"labelValue\"].to(device)                         #Loading label values to GPU\n","            textSrcInre            = d[\"TextSrcInre\"]\n","            tfidf_transform        = x_train_feats.transform(textSrcInre)\n","            tfidf_transform_tensor = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_transform)).float()    \n","\n","            pca_tensor             = p.transform(tfidf_transform_tensor)\n","\n","            pca_tensor = torch.from_numpy(pca_tensor).float()\n","            pca_tensor = pca_tensor.to(device)\n","            tfidf_transform_tensor = tfidf_transform_tensor.to(device)\n","\n","            #Getting the softmax output from model for dev data\n","            outputs = model(\n","            input_ids             = input_ids,\n","            attention_mask        = attention_mask,\n","            inputs_tfidf_feats    = tfidf_transform_tensor,\n","            pca_transformed_feats = pca_tensor,\n","            modelType             = 'roberta-TrainedTfIdf'\n","            )\n","\n","            #Determining the model predictions\n","            _, predictionIndices = torch.max(outputs, dim=1)\n","            loss = lossFunction(outputs, labelValues)\n","\n","            #Calculating the correct predictions for accuracy\n","            correctPredictions += torch.sum(predictionIndices == labelValues)\n","            losses.append(loss.item())\n","\n","    return np.mean(losses), correctPredictions.double() / n_examples\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2tbWMvslJH0","executionInfo":{"status":"ok","timestamp":1648892579524,"user_tz":-120,"elapsed":2492608,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dc49d3e2-5c6f-4d04-ddfd-9dfc12844c27"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["Training loss 0.49331903172720587 Training accuracy 0.8668411867364747\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Development loss 0.47815606516340503 Development accuracy 0.875494071146245\n","\n","\n","Epoch 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["Training loss 0.4728093569876011 Training accuracy 0.8792321116928448\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Development loss 0.48174248464965064 Development accuracy 0.875494071146245\n","\n","\n","Epoch 3\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["Training loss 0.48200922345499514 Training accuracy 0.8877835951134381\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Development loss 0.47776027100359497 Development accuracy 0.8962450592885375\n","\n","\n","Epoch 4\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["Training loss 0.46928191164200406 Training accuracy 0.899825479930192\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Development loss 0.45007069461901666 Development accuracy 0.91699604743083\n","\n","\n","Epoch 5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["Training loss 0.4407467870594152 Training accuracy 0.9197207678883073\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Development loss 0.4377692511430371 Development accuracy 0.9229249011857706\n","\n","\n","Epoch 6\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["Training loss 0.4206650354233307 Training accuracy 0.9289703315881327\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Development loss 0.4312137328824507 Development accuracy 0.924901185770751\n","\n","\n","Epoch 7\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["Training loss 0.4193476529405904 Training accuracy 0.9338568935427575\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Development loss 0.44239405987291 Development accuracy 0.9229249011857706\n","\n","\n"]}],"source":["#fine tuning ROBERTa and validating it \n","\n","for epoch in range(EPOCHS):\n","    print(f'Epoch {epoch + 1}')\n","    trainLoss, trainAccuracy = train_epoch(\n","        model,\n","        trainDataLoader,\n","        lossFunction,\n","        optimizer,\n","        device,\n","        scheduler,\n","        len(trainDf)\n","      )\n","    print(f'Training loss {trainLoss} Training accuracy {trainAccuracy}')\n","    devLoss, devAccuracy = eval_model(\n","        model,\n","        developmentDataLoader,\n","        lossFunction,\n","        device,\n","        len(devDf)\n","      )\n","    print(f'Development loss {devLoss} Development accuracy {devAccuracy}')\n","    print()\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3cvsigzislBE"},"outputs":[],"source":["#This function gets the predictions from the model after it is trained.\n","def get_predictions(model, data_loader):\n","\n","    model = model.eval()\n","    review_texta = []\n","#     review_textb = []               #     !! Change - commented\n","    predictions = []\n","    prediction_probs = []\n","    real_values = []\n","\n","    with torch.no_grad():\n","        for d in data_loader:\n","\n","            textSrcInre                 = d[\"TextSrcInre\"]\n","#             textbs                 = d[\"secondSeq\"]\n","            input_ids              = d[\"input_ids\"].to(device)\n","            attention_mask         = d[\"attention_mask\"].to(device)\n","            labels                 = d[\"labelValue\"].to(device)\n","            # Features            = d[\"Features\"]\n","            tfidf_transform        = tfidf.transform(textSrcInre)\n","            tfidf_transform_tensor = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_transform)).float()\n","\n","            pca_tensor             =  p.transform(tfidf_transform_tensor)\n","\n","            pca_tensor = torch.from_numpy(pca_tensor).float()\n","            pca_tensor = pca_tensor.to(device)\n","            tfidf_transform_tensor = tfidf_transform_tensor.to(device)\n","\n","            #Getting the softmax output from model\n","            outputs = model(\n","                input_ids             = input_ids,\n","                attention_mask        = attention_mask,\n","                inputs_tfidf_feats    = tfidf_transform_tensor,\n","                pca_transformed_feats = pca_tensor,\n","                modelType             = 'roberta-TrainedTfIdf'\n","                )\n","            _, preds = torch.max(outputs, dim=1)     #Determining the model predictions\n","\n","            review_texta.extend(textSrcInre)\n","#             review_textb.extend(textbs)\n","            predictions.extend(preds)\n","            prediction_probs.extend(outputs)\n","            real_values.extend(labels)\n","    predictions = torch.stack(predictions).cpu()\n","    prediction_probs = torch.stack(prediction_probs).cpu()\n","    real_values = torch.stack(real_values).cpu()\n","  \n","    return review_texta, predictions, prediction_probs, real_values\n","#    return review_texta, review_textb, predictions, prediction_probs, real_values\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0eCWXLuvslTG","executionInfo":{"status":"ok","timestamp":1648892595248,"user_tz":-120,"elapsed":15729,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c66247fd-4ffc-4393-916f-34340b8859d1"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}],"source":["#Getting model predictions on dev dataset\n","# firstSeq_dev, secondSeq_dev, yHat_dev, predProbs_dev, yTest_dev = get_predictions(\n","#   model,\n","#   developmentDataLoader\n","# )\n","\n","firstSeq_dev, yHat_dev, predProbs_dev, yTest_dev = get_predictions(\n","  model,\n","  developmentDataLoader\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kOufuWqIslba","executionInfo":{"status":"ok","timestamp":1648892595249,"user_tz":-120,"elapsed":46,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e354b7ce-9ff5-4893-fda1-c27c6d86bb0a"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","     Non-ADU     0.8219    0.6977    0.7547       172\n","         ADU     0.9400    0.9690    0.9543       840\n","\n","    accuracy                         0.9229      1012\n","   macro avg     0.8809    0.8334    0.8545      1012\n","weighted avg     0.9199    0.9229    0.9204      1012\n","\n","[[120  52]\n"," [ 26 814]]\n"]}],"source":[" #Printing classification report for dev dataset (Evaluating the model on Dev set)\n","print(classification_report(yTest_dev, yHat_dev, target_names= CLASS_NAMES,digits=4))\n","print(confusion_matrix(yTest_dev, yHat_dev))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3XMaZRFCslel"},"outputs":[],"source":["# torch.save(model.state_dict(),f'RoBERTaLarge_TFIDFV2.pt')\n","\n","# #for collab \n","# #Saving the model onto the drive\n","# from google.colab import drive\n","# drive.mount('/content/gdrive')\n","\n","# model_save_name = 'RoBERTaLarge_TFIDF_cmv_Step1.pt'\n","# path = F\"/content/gdrive/My Drive/Colab Notebooks/{model_save_name}\" \n","# torch.save(model.state_dict(), path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kbfBps65st6u","executionInfo":{"status":"ok","timestamp":1648892608855,"user_tz":-120,"elapsed":13646,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6806fce8-87cb-48b4-f57e-ffce08a020be"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}],"source":["#Getting model predictions on test dataset\n","firstSeq_test, yHat_test, predProbs_test, yTest_test = get_predictions(\n","  model,\n","  testDataLoader\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECQy76TAst_i","executionInfo":{"status":"ok","timestamp":1648892608856,"user_tz":-120,"elapsed":18,"user":{"displayName":"Garima Mudgal","userId":"14901317976461361357"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cb3db737-49dd-4ce1-cb7a-185a7b030341"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","     Non-ADU     0.8543    0.7679    0.8088       168\n","         ADU     0.9471    0.9694    0.9581       720\n","\n","    accuracy                         0.9313       888\n","   macro avg     0.9007    0.8687    0.8835       888\n","weighted avg     0.9295    0.9313    0.9299       888\n","\n","[[129  39]\n"," [ 22 698]]\n"]}],"source":["#Printing classification report for test dataset (Evaluating the model on test set)\n","print(classification_report(yTest_test, yHat_test, target_names= CLASS_NAMES,digits=4))\n","print(confusion_matrix(yTest_test, yHat_test))"]},{"cell_type":"markdown","metadata":{"id":"3UWlrWijdtXp"},"source":["**Prediction Starts here**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g987PDxVbsHU"},"outputs":[],"source":["#Creates a dataset which will be used to feed to RoBERTa\n","class StanceIcleDataset(data.Dataset):\n","  def __init__(self,TextSrcInre,  tokenizer, max_len):\n","        self.TextSrcInre = TextSrcInre   #Concatenation of reply+ previous+ src text to get features from 1 training example\n","        self.tokenizer   = tokenizer     #tokenizer that will be used to tokenize input sequences (Uses BERT-tokenizer here)\n","        self.max_len     = max_len       #Maximum length of the tokens from the input sequence that BERT needs to attend to\n","\n","  def __len__(self):\n","        return len(self.TextSrcInre)\n","\n","  def __getitem__(self, item):\n","        # firstSeq    = str(self.firstSeq[item])\n","        # secondSeq   = str(self.secondSeq[item])\n","        TextSrcInre = str(self.TextSrcInre[item])\n","\n","    #Encoding the first and the second sequence to a form accepted by RoBERTa\n","    #RoBERTa does not use token_type_ids to distinguish the first sequence from the second sequnece.\n","        encoding = tokenizer.encode_plus(\n","            TextSrcInre,\n","            max_length = self.max_len,\n","            add_special_tokens= True,\n","            truncation = True,\n","            pad_to_max_length = True,\n","            return_attention_mask = True,\n","            return_tensors = 'pt'\n","        )\n","\n","        return {\n","            'TextSrcInre': TextSrcInre,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","        }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWqpDGr7bsdR"},"outputs":[],"source":["#Creates a data loader\n","def createIcleDataLoader(dataframe, tokenizer, max_len, batch_size):\n","    ds = StanceIcleDataset(\n","        TextSrcInre = dataframe.Text.to_numpy(),\n","        tokenizer   = tokenizer,\n","        max_len     = max_len\n","    )\n","\n","    return data.DataLoader(\n","        ds,\n","        batch_size  = batch_size,\n","        shuffle     = False,\n","        num_workers = 2\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmeWZGXZvn5X"},"outputs":[],"source":["#This function gets the predictions from the model after it is trained.\n","def get_icle_predictions(model, data_loader):\n","\n","    model = model.eval()\n","    review_texta = []\n","    predictions = []\n","    prediction_probs = []\n","    real_values = []\n","\n","    with torch.no_grad():\n","        for d in data_loader:\n","\n","            textSrcInre                 = d[\"TextSrcInre\"]\n","            input_ids              = d[\"input_ids\"].to(device)\n","            attention_mask         = d[\"attention_mask\"].to(device)\n","            tfidf_transform        = tfidf.transform(textSrcInre)\n","            tfidf_transform_tensor = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_transform)).float()\n","\n","            pca_tensor             =  p.transform(tfidf_transform_tensor)\n","\n","            pca_tensor = torch.from_numpy(pca_tensor).float()\n","            pca_tensor = pca_tensor.to(device)\n","            tfidf_transform_tensor = tfidf_transform_tensor.to(device)\n","\n","            #Getting the softmax output from model\n","            outputs = model(\n","                input_ids             = input_ids,\n","                attention_mask        = attention_mask,\n","                inputs_tfidf_feats    = tfidf_transform_tensor,\n","                pca_transformed_feats = pca_tensor,\n","                modelType             = 'roberta-TrainedTfIdf'\n","                )\n","            _, preds = torch.max(outputs, dim=1)     #Determining the model predictions\n","\n","            review_texta.extend(textSrcInre)\n","            predictions.extend(preds)\n","            prediction_probs.extend(outputs)\n","    predictions = torch.stack(predictions).cpu()\n","    prediction_probs = torch.stack(prediction_probs).cpu()\n","  \n","    return review_texta, predictions, prediction_probs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8L9JxTyFeChw"},"outputs":[],"source":["def int_to_label(label):\n","    if label   == 0:\n","        return 'Non-ADU'\n","    elif label == 1:\n","        return 'ADU'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"al_zQk9qeCji"},"outputs":[],"source":["#Creating data loader for ICLE 5085 data\n","# path2= F\"/content/gdrive/My Drive/Colab Notebooks/1. ADU_Classification/ICLE_5085.xlsx\" \n","path2= F\"/content/gdrive/My Drive/Colab Notebooks/1. ADU_Classification/Annotations_persing_ICLE_1000.xlsx\" \n","icle_df=pd.read_excel(path2,sheet_name='ADU_Classification',usecols=['Text'])\n","\n","ICLE_DataLoader  = createIcleDataLoader(icle_df, tokenizer, MAX_LENGTH, BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DXuAHhN7eH13","executionInfo":{"status":"ok","timestamp":1645626553037,"user_tz":-60,"elapsed":435641,"user":{"displayName":"Garima Mudgal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihbN2FEuxMGO0JqJ3fdQhmaS3HMx15Ipgmo6tbPQ=s64","userId":"14901317976461361357"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ee2070d9-9a64-4d15-caf5-4d8cf6c22684"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}],"source":["firstSeq_ICLE, yHat_icle, predProbs_icle = get_icle_predictions(\n","  model,\n","  ICLE_DataLoader\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2y6BHCmaeIkm","executionInfo":{"status":"ok","timestamp":1645626553038,"user_tz":-60,"elapsed":11,"user":{"displayName":"Garima Mudgal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihbN2FEuxMGO0JqJ3fdQhmaS3HMx15Ipgmo6tbPQ=s64","userId":"14901317976461361357"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"beed721f-d384-4f84-9c90-c0166a1bc31f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 1])"]},"metadata":{},"execution_count":34}],"source":["# firstSeq_ICLE\n","yHat_icle.unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ua8u99EGeItR","executionInfo":{"status":"ok","timestamp":1645626556979,"user_tz":-60,"elapsed":3944,"user":{"displayName":"Garima Mudgal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihbN2FEuxMGO0JqJ3fdQhmaS3HMx15Ipgmo6tbPQ=s64","userId":"14901317976461361357"}},"colab":{"base_uri":"https://localhost:8080/","height":16},"outputId":"14692e40-df7a-4838-ae02-f2392f2d1ae1"},"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_f55f12b1-561c-48c0-bab6-c4d97e3dda1b\", \"ICLE_ADU_NonADU.xlsx\", 1884942)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}}],"source":["from google.colab import files \n","\n","predictions=pd.DataFrame(list(zip(firstSeq_ICLE, yHat_icle.numpy())),columns=['Text','Ann_Adu'])\n","predictions['Ann_Adu']=predictions.Ann_Adu.apply(int_to_label)\n","predictions['Ann_Adu'].value_counts()\n","predictions.to_excel('ICLE_ADU_NonADU.xlsx')\n","files.download('ICLE_ADU_NonADU.xlsx')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MPB8Zx5DFl44"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tSqRrym4Fl8S"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"31.03.2022Step1_Roberta_TFIDF_Attempt3_CMV+AAE_Adu_NonAdu.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"674ba2c030ed493c9e4643c11feaba69":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c927bf88f30e48cfa8c1625b1a716e0a","IPY_MODEL_1e34216bc43c46e2b57f9e2f05baef84","IPY_MODEL_48c46337776b48d6aebf1f598c10441c"],"layout":"IPY_MODEL_0b076874c3da4cf283c67a4c0821aeaf"}},"c927bf88f30e48cfa8c1625b1a716e0a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c173a92f32e438fbfb25e03c1f9a8fd","placeholder":"​","style":"IPY_MODEL_f57d79e32f5845abacc6b7efe343ed74","value":"Downloading: 100%"}},"1e34216bc43c46e2b57f9e2f05baef84":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d27d2405ea148898341e53521418df1","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6cadfb36a61e4ba894143320fdef92a7","value":898823}},"48c46337776b48d6aebf1f598c10441c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e4f740cb5354a17bf4cc7ba5c716f36","placeholder":"​","style":"IPY_MODEL_c4cb703c21f349a5b27d6f625b0daa73","value":" 878k/878k [00:00&lt;00:00, 3.46MB/s]"}},"0b076874c3da4cf283c67a4c0821aeaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c173a92f32e438fbfb25e03c1f9a8fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f57d79e32f5845abacc6b7efe343ed74":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d27d2405ea148898341e53521418df1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cadfb36a61e4ba894143320fdef92a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6e4f740cb5354a17bf4cc7ba5c716f36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4cb703c21f349a5b27d6f625b0daa73":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0aef75cc478244d88a86e8368b75ea8e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_29a15cd1b8364e4e8dbd1e96515e4c3f","IPY_MODEL_0d67ded6e63442deaa03fe00ffd7ebed","IPY_MODEL_c16f19a12ceb45b183ae3c27f5218ec9"],"layout":"IPY_MODEL_5e73a639554644adb04814aecde8087b"}},"29a15cd1b8364e4e8dbd1e96515e4c3f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_929587c2ad044ca29f57098fabbac91e","placeholder":"​","style":"IPY_MODEL_4374479c68924f1393c0714bee792163","value":"Downloading: 100%"}},"0d67ded6e63442deaa03fe00ffd7ebed":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b1a4027003c4473baab083bfd13d814","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1c7d1f1fd58a4ceca29017dd7caae75a","value":456318}},"c16f19a12ceb45b183ae3c27f5218ec9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7712343ea5a4c3da8590029b66b0c3c","placeholder":"​","style":"IPY_MODEL_e76cc33376b44f5da8740cddf7ff8c9b","value":" 446k/446k [00:00&lt;00:00, 3.84MB/s]"}},"5e73a639554644adb04814aecde8087b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"929587c2ad044ca29f57098fabbac91e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4374479c68924f1393c0714bee792163":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b1a4027003c4473baab083bfd13d814":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c7d1f1fd58a4ceca29017dd7caae75a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7712343ea5a4c3da8590029b66b0c3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e76cc33376b44f5da8740cddf7ff8c9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0934d86f06dd45978a85be3919b5b1af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_36d25d559d874a28b68c02bdb468b47d","IPY_MODEL_c23c3b06f7c14194b51cd63463da3eca","IPY_MODEL_e8079551a42a436ebfdcea7a05d99c26"],"layout":"IPY_MODEL_e6fc2b8b2a534e2fb9181bc8bc983cf4"}},"36d25d559d874a28b68c02bdb468b47d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32791dcd164e4d0ea4276069220859f9","placeholder":"​","style":"IPY_MODEL_70848cdd13794187851cc638233d6403","value":"Downloading: 100%"}},"c23c3b06f7c14194b51cd63463da3eca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_160231b694da4a058faca755cd138a40","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3377e97c4ec14ec6adf095fcab17c1e9","value":482}},"e8079551a42a436ebfdcea7a05d99c26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13f779e5a9ce40e69370402f2f0ab1d9","placeholder":"​","style":"IPY_MODEL_90276d0ddac2494e9b97e1f0ca6414a7","value":" 482/482 [00:00&lt;00:00, 14.5kB/s]"}},"e6fc2b8b2a534e2fb9181bc8bc983cf4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32791dcd164e4d0ea4276069220859f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70848cdd13794187851cc638233d6403":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"160231b694da4a058faca755cd138a40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3377e97c4ec14ec6adf095fcab17c1e9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"13f779e5a9ce40e69370402f2f0ab1d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90276d0ddac2494e9b97e1f0ca6414a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"27a3194603264f729f256fa3a0692eb1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c0e5b3289af54e2eb9551d0a81a0642d","IPY_MODEL_0de4a1a2145846b5a8ce5c3f5fcb93ba","IPY_MODEL_fbb23d2d51d4419ca7bbb05e8f892d2b"],"layout":"IPY_MODEL_58b3f9e6bfe04887a7adf2e4ba1abba7"}},"c0e5b3289af54e2eb9551d0a81a0642d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a42afc24f69f4dcb9d275ecc7a36b991","placeholder":"​","style":"IPY_MODEL_2356f82b8d4c4393850faf666fa84267","value":"Downloading: 100%"}},"0de4a1a2145846b5a8ce5c3f5fcb93ba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8b5af16468d4275bffb7f6789940d93","max":1425941629,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1600b6fad2d94e7182a0b699ba395176","value":1425941629}},"fbb23d2d51d4419ca7bbb05e8f892d2b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_baa55523bba04355b2ceea3dd7d26354","placeholder":"​","style":"IPY_MODEL_fa152919c8444b04a796efc4d8648e74","value":" 1.33G/1.33G [00:38&lt;00:00, 42.1MB/s]"}},"58b3f9e6bfe04887a7adf2e4ba1abba7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a42afc24f69f4dcb9d275ecc7a36b991":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2356f82b8d4c4393850faf666fa84267":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a8b5af16468d4275bffb7f6789940d93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1600b6fad2d94e7182a0b699ba395176":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"baa55523bba04355b2ceea3dd7d26354":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa152919c8444b04a796efc4d8648e74":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}